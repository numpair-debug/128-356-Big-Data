{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "cell_0",
      "metadata": {
        "id": "cell_0"
      },
      "source": [
        "# üìò Big Data ‡∏™‡∏±‡∏õ‡∏î‡∏≤‡∏´‡πå‡∏ó‡∏µ‡πà 4: Distributed Processing & Apache Spark (Deep Dive)\n",
        "\n",
        "\n",
        "**‡∏™‡∏±‡∏õ‡∏î‡∏≤‡∏´‡πå‡∏ó‡∏µ‡πà:** 4  \n",
        "\n",
        "> üéØ **‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ:** ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡πÅ‡∏ô‡∏ß‡∏Ñ‡∏¥‡∏î Distributed Computing, ‡∏™‡∏ñ‡∏≤‡∏õ‡∏±‡∏ï‡∏¢‡∏Å‡∏£‡∏£‡∏°‡∏Ç‡∏≠‡∏á Apache Spark, Lazy Evaluation, DAG ‡πÅ‡∏•‡∏∞ Shuffle ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏•‡∏á‡∏°‡∏∑‡∏≠‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡∏à‡∏£‡∏¥‡∏á‡∏î‡πâ‡∏ß‡∏¢ PySpark\n",
        "\n",
        "**‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ‡∏ó‡∏±‡πâ‡∏á**: Google Colab (‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥) / Local Jupyter  \n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell_1",
      "metadata": {
        "id": "cell_1"
      },
      "source": [
        "# 1. üñ•Ô∏è ‡∏ó‡∏≥‡πÑ‡∏°‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏ñ‡∏∂‡∏á‡πÑ‡∏°‡πà‡∏û‡∏≠? (Why Single Machine Fails)\n",
        "\n",
        "## ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏î‡πâ‡∏ß‡∏¢‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß\n",
        "\n",
        "‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏°‡∏µ‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà‡∏Ç‡∏∂‡πâ‡∏ô ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏´‡∏•‡∏≤‡∏¢‡∏î‡πâ‡∏≤‡∏ô:\n",
        "\n",
        "| ‡∏Ç‡πâ‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î | ‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢ | ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏õ‡∏±‡∏ç‡∏´‡∏≤ |\n",
        "|----------|----------|---------------|\n",
        "| **RAM** | ‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≥‡∏°‡∏µ‡∏à‡∏≥‡∏Å‡∏±‡∏î (‡πÄ‡∏ä‡πà‡∏ô 16 GB) | ‡∏•‡∏≠‡∏á‡πÇ‡∏´‡∏•‡∏î CSV 50 GB ‡∏î‡πâ‡∏ß‡∏¢ Pandas ‚Üí MemoryError |\n",
        "| **CPU** | ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô Core ‡∏°‡∏µ‡∏à‡∏≥‡∏Å‡∏±‡∏î (‡πÄ‡∏ä‡πà‡∏ô 4-8 Cores) | ‡∏á‡∏≤‡∏ô groupBy ‡∏ö‡∏ô 1 ‡∏û‡∏±‡∏ô‡∏•‡πâ‡∏≤‡∏ô‡πÅ‡∏ñ‡∏ß ‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤‡∏´‡∏•‡∏≤‡∏¢‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á |\n",
        "| **I/O** | Disk ‡∏≠‡πà‡∏≤‡∏ô/‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏ä‡πâ‡∏≤ | ‡∏™‡πÅ‡∏Å‡∏ô‡πÑ‡∏ü‡∏•‡πå 100 GB ‡∏à‡∏≤‡∏Å HDD ‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤ ~15 ‡∏ô‡∏≤‡∏ó‡∏µ |\n",
        "| **‡πÑ‡∏°‡πà‡∏Ç‡∏¢‡∏≤‡∏¢‡πÑ‡∏î‡πâ** | ‡πÄ‡∏û‡∏¥‡πà‡∏° RAM/CPU ‡∏°‡∏µ‡πÄ‡∏û‡∏î‡∏≤‡∏ô | ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÅ‡∏û‡∏á‡∏™‡∏∏‡∏î‡πÉ‡∏ô‡∏ï‡∏•‡∏≤‡∏î‡∏Å‡πá‡∏°‡∏µ‡∏Ç‡∏µ‡∏î‡∏à‡∏≥‡∏Å‡∏±‡∏î |\n",
        "\n",
        "### üí° ‡πÅ‡∏ô‡∏ß‡∏Ñ‡∏¥‡∏î‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç (Key Insight)\n",
        "\n",
        "> ‡πÄ‡∏°‡∏∑‡πà‡∏≠ **‡∏Ç‡∏ô‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• √ó ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô > ‡∏Ç‡∏µ‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏Ç‡∏≠‡∏á‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß**  \n",
        "> ‚Üí ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ **Distributed Computing** (‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏ö‡∏ö‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢)\n",
        "\n",
        "### ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏á‡πà‡∏≤‡∏¢ ‡πÜ\n",
        "\n",
        "- **‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß** = ‡∏Ñ‡∏ô‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏¢‡∏Å‡∏≠‡∏¥‡∏ê 10,000 ‡∏Å‡πâ‡∏≠‡∏ô ‚Üí ‡πÄ‡∏´‡∏ô‡∏∑‡πà‡∏≠‡∏¢ ‡∏ä‡πâ‡∏≤ ‡πÄ‡∏™‡∏µ‡∏¢‡πÄ‡∏ß‡∏•‡∏≤\n",
        "- **Distributed** = ‡∏Ñ‡∏ô 100 ‡∏Ñ‡∏ô ‡∏ä‡πà‡∏ß‡∏¢‡∏Å‡∏±‡∏ô‡∏¢‡∏Å‡∏Ñ‡∏ô‡∏•‡∏∞ 100 ‡∏Å‡πâ‡∏≠‡∏ô ‚Üí ‡πÄ‡∏™‡∏£‡πá‡∏à‡πÄ‡∏£‡πá‡∏ß!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "img_single_vs_distributed",
      "metadata": {
        "id": "img_single_vs_distributed"
      },
      "source": [
        "<div align=\"center\">\n",
        "  <img src=\"https://github.com/witsarutsarai12-Academic/128-356-Big-Data/blob/main/images/single_vs_distributed.png?raw=1\" width=\"800\" alt=\"Single Machine vs Distributed Computing: ‡∏Ñ‡∏ô‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡πÅ‡∏ö‡∏Å‡∏Ç‡∏≠‡∏á‡∏´‡∏ô‡∏±‡∏Å vs ‡∏´‡∏•‡∏≤‡∏¢‡∏Ñ‡∏ô‡∏ä‡πà‡∏ß‡∏¢‡∏Å‡∏±‡∏ô‡πÅ‡∏ö‡∏Å\">\n",
        "  <br><i>Single Machine vs Distributed Computing: ‡∏Ñ‡∏ô‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡πÅ‡∏ö‡∏Å‡∏Ç‡∏≠‡∏á‡∏´‡∏ô‡∏±‡∏Å vs ‡∏´‡∏•‡∏≤‡∏¢‡∏Ñ‡∏ô‡∏ä‡πà‡∏ß‡∏¢‡∏Å‡∏±‡∏ô‡πÅ‡∏ö‡∏Å</i>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell_2",
      "metadata": {
        "id": "cell_2"
      },
      "outputs": [],
      "source": [
        "# ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: ‡∏•‡∏≠‡∏á‡∏î‡∏π‡∏Ç‡πâ‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏Ç‡∏≠‡∏á‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß\n",
        "import psutil, os\n",
        "\n",
        "ram_gb = psutil.virtual_memory().total / (1024**3)\n",
        "cpu_count = os.cpu_count()\n",
        "\n",
        "print(f\"üñ•Ô∏è ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏ô‡∏µ‡πâ‡∏°‡∏µ RAM: {ram_gb:.1f} GB\")\n",
        "print(f\"üîß ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô CPU Cores: {cpu_count}\")\n",
        "print(f\"\\nüí° ‡∏ñ‡πâ‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏ç‡πà‡∏Å‡∏ß‡πà‡∏≤ {ram_gb:.0f} GB ‚Üí Pandas ‡∏à‡∏∞ crash!\")\n",
        "print(f\"üí° ‡∏ñ‡πâ‡∏≤ query ‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô ‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ‡πÅ‡∏Ñ‡πà {cpu_count} cores ‚Üí ‡∏ä‡πâ‡∏≤!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell_3",
      "metadata": {
        "id": "cell_3"
      },
      "source": [
        "### ‚úèÔ∏è ‡πÅ‡∏ö‡∏ö‡∏ù‡∏∂‡∏Å‡∏´‡∏±‡∏î‡∏ó‡∏µ‡πà 1: ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ç‡∏µ‡∏î‡∏à‡∏≥‡∏Å‡∏±‡∏î\n",
        "\n",
        "**‡πÇ‡∏à‡∏ó‡∏¢‡πå:** ‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Log 500 GB/‡∏ß‡∏±‡∏ô ‡∏ï‡πâ‡∏≠‡∏á JOIN ‡∏Å‡∏±‡∏ö‡∏ï‡∏≤‡∏£‡∏≤‡∏á 50 GB ‡πÅ‡∏•‡∏∞ GROUP BY  \n",
        "‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á Server ‡∏°‡∏µ RAM 64 GB, 16 Cores\n",
        "\n",
        "**‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°** (‡πÄ‡∏ï‡∏¥‡∏°‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡πÉ‡∏ô‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á):\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "cell_4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cell_4",
        "outputId": "d6ebb043-49db-449c-909e-98b70e2b5b05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏£‡∏ß‡∏°: 550 GB\n",
            "RAM ‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠: False\n",
            "‡πÄ‡∏ß‡∏•‡∏≤‡∏≠‡πà‡∏≤‡∏ô disk: 2750 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ ‚âà 45.8 ‡∏ô‡∏≤‡∏ó‡∏µ\n",
            "‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö: Yes\n"
          ]
        }
      ],
      "source": [
        "# ‡πÅ‡∏ö‡∏ö‡∏ù‡∏∂‡∏Å‡∏´‡∏±‡∏î‡∏ó‡∏µ‡πà 1: ‡πÄ‡∏ï‡∏¥‡∏°‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö\n",
        "\n",
        "# 1) ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏£‡∏ß‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏Å‡∏µ‡πà GB?\n",
        "total_data_gb = 550  # ‡πÄ‡∏ï‡∏¥‡∏°‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç\n",
        "\n",
        "# 2) RAM ‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà? (True/False)\n",
        "ram_enough = False  # True ‡∏´‡∏£‡∏∑‡∏≠ False\n",
        "\n",
        "# 3) ‡∏ñ‡πâ‡∏≤‡∏≠‡πà‡∏≤‡∏ô disk ‡πÑ‡∏î‡πâ 200 MB/s ‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤‡∏ô‡∏≤‡∏ô‡∏Å‡∏µ‡πà‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ (‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì)?\n",
        "read_time_sec = 2750  # ‡πÄ‡∏ï‡∏¥‡∏°‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç\n",
        "\n",
        "# 4) ‡∏Ñ‡∏ß‡∏£‡πÉ‡∏ä‡πâ Distributed Computing ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà? ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏≠‡∏∞‡πÑ‡∏£?\n",
        "answer = \"Yes\"\n",
        "\n",
        "print(f\"‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏£‡∏ß‡∏°: {total_data_gb} GB\")\n",
        "print(f\"RAM ‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠: {ram_enough}\")\n",
        "print(f\"‡πÄ‡∏ß‡∏•‡∏≤‡∏≠‡πà‡∏≤‡∏ô disk: {read_time_sec} ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ ‚âà {read_time_sec/60:.1f} ‡∏ô‡∏≤‡∏ó‡∏µ\")\n",
        "print(f\"‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö: {answer}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell_5",
      "metadata": {
        "id": "cell_5"
      },
      "source": [
        "# 2. üî• Apache Spark ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£?\n",
        "\n",
        "## ‡∏ô‡∏¥‡∏¢‡∏≤‡∏°\n",
        "\n",
        "Apache Spark ‡∏Ñ‡∏∑‡∏≠ **Distributed Computing Engine** (‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏¢‡∏ô‡∏ï‡πå‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏ö‡∏ö‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢) ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà\n",
        "\n",
        "> Spark ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô \"‡∏™‡∏°‡∏≠‡∏á‡∏Å‡∏•‡∏≤‡∏á\" ‡∏ó‡∏µ‡πà‡∏™‡∏±‡πà‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏•‡∏≤‡∏¢‡∏£‡πâ‡∏≠‡∏¢‡∏ï‡∏±‡∏ß‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏±‡∏ô\n",
        "\n",
        "## ‡∏Ñ‡∏∏‡∏ì‡∏™‡∏°‡∏ö‡∏±‡∏ï‡∏¥‡∏´‡∏•‡∏±‡∏Å\n",
        "\n",
        "| ‡∏Ñ‡∏∏‡∏ì‡∏™‡∏°‡∏ö‡∏±‡∏ï‡∏¥ | ‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢ | ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö |\n",
        "|-----------|----------|-------------|\n",
        "| **In-Memory Processing** | ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÉ‡∏ô RAM ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô disk ‡∏ó‡∏∏‡∏Å‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô | ‡πÄ‡∏£‡πá‡∏ß‡∏Å‡∏ß‡πà‡∏≤ Hadoop MapReduce 10-100 ‡πÄ‡∏ó‡πà‡∏≤ |\n",
        "| **DAG Execution** | ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏ú‡∏ô‡∏á‡∏≤‡∏ô‡πÄ‡∏õ‡πá‡∏ô Graph ‡∏Å‡πà‡∏≠‡∏ô‡∏ó‡∏≥‡∏à‡∏£‡∏¥‡∏á | ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏ß‡∏≤‡∏á‡πÅ‡∏ú‡∏ô‡∏Å‡πà‡∏≠‡∏ô‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ö‡πâ‡∏≤‡∏ô |\n",
        "| **Fault Tolerance** | ‡∏ñ‡πâ‡∏≤‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏û‡∏±‡∏á ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÉ‡∏´‡∏°‡πà‡πÑ‡∏î‡πâ | ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏°‡∏µ‡∏™‡∏≥‡πÄ‡∏ô‡∏≤‡∏™‡∏π‡∏ï‡∏£‡∏≠‡∏≤‡∏´‡∏≤‡∏£ |\n",
        "| **Unified Engine** | ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö SQL, ML, Streaming, Graph | ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏°‡∏∑‡∏≠‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏ó‡∏≥‡πÑ‡∏î‡πâ‡∏´‡∏•‡∏≤‡∏¢‡∏≠‡∏¢‡πà‡∏≤‡∏á |\n",
        "\n",
        "## ‚ö†Ô∏è Spark ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡∏≠‡∏∞‡πÑ‡∏£?\n",
        "\n",
        "- ‚ùå **‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà Database** ‚Äî Spark ‡πÑ‡∏°‡πà‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ñ‡∏≤‡∏ß‡∏£\n",
        "- ‚ùå **‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà Storage System** ‚Äî Spark ‡∏≠‡πà‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å HDFS, S3, Parquet ‡∏Ø‡∏•‡∏Ø\n",
        "- ‚úÖ **Spark = Compute Layer** ‚Äî ‡πÄ‡∏õ‡πá‡∏ô \"‡∏ä‡∏±‡πâ‡∏ô‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•\" ‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏±‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå\n",
        "\n",
        "### ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö Spark ‡∏Å‡∏±‡∏ö‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏°‡∏∑‡∏≠‡∏≠‡∏∑‡πà‡∏ô\n",
        "\n",
        "| ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏°‡∏∑‡∏≠ | ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö | ‡∏Ç‡∏ô‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• | ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß |\n",
        "|-----------|---------|-----------|---------|\n",
        "| **Pandas** | ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß, EDA | MB - ‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô RAM | ‡πÄ‡∏£‡πá‡∏ß (‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏•‡πá‡∏Å) |\n",
        "| **DuckDB** | SQL Analytics ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß | MB - GB | ‡πÄ‡∏£‡πá‡∏ß‡∏°‡∏≤‡∏Å |\n",
        "| **Spark** | Distributed, Data Pipeline | GB - PB | ‡πÄ‡∏£‡πá‡∏ß (‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏ç‡πà) |\n",
        "| **Hadoop MR** | Batch ‡∏ö‡∏ô HDFS | TB - PB | ‡∏ä‡πâ‡∏≤ (‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô disk ‡∏ó‡∏∏‡∏Å‡∏Ç‡∏±‡πâ‡∏ô) |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell_6",
      "metadata": {
        "id": "cell_6"
      },
      "source": [
        "# 3. üèóÔ∏è ‡∏™‡∏ñ‡∏≤‡∏õ‡∏±‡∏ï‡∏¢‡∏Å‡∏£‡∏£‡∏° Spark ‡πÄ‡∏ä‡∏¥‡∏á‡∏•‡∏∂‡∏Å (Spark Architecture Deep Dive)\n",
        "\n",
        "## ‡∏™‡πà‡∏ß‡∏ô‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏´‡∏•‡∏±‡∏Å 3 ‡∏™‡πà‡∏ß‡∏ô\n",
        "\n",
        "```\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ         Cluster Manager             ‚îÇ\n",
        "‚îÇ    (‡∏ú‡∏π‡πâ‡∏à‡∏±‡∏î‡∏™‡∏£‡∏£‡∏ó‡∏£‡∏±‡∏û‡∏¢‡∏≤‡∏Å‡∏£: YARN/K8s)     ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "           ‚îÇ ‡∏à‡∏±‡∏î‡∏™‡∏£‡∏£ CPU/RAM\n",
        "    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "    ‚ñº             ‚ñº\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ Driver ‚îÇ‚îÄ‚îÄ‚ñ∂‚îÇ   Executors (N ‡∏ï‡∏±‡∏ß) ‚îÇ\n",
        "‚îÇ (‡∏™‡∏°‡∏≠‡∏á) ‚îÇ   ‚îÇ   (‡πÅ‡∏£‡∏á‡∏á‡∏≤‡∏ô)         ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "```\n",
        "\n",
        "### üß† Driver (‡∏™‡∏°‡∏≠‡∏á) ‚Äî ‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà:\n",
        "1. **‡∏™‡∏£‡πâ‡∏≤‡∏á SparkSession** ‚Äî ‡πÄ‡∏õ‡∏¥‡∏î‡∏õ‡∏£‡∏∞‡∏ï‡∏π‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏π‡πà Spark\n",
        "2. **‡∏™‡∏£‡πâ‡∏≤‡∏á Logical Plan** ‚Äî ‡πÅ‡∏õ‡∏•‡∏á‡πÇ‡∏Ñ‡πâ‡∏î‡πÄ‡∏õ‡πá‡∏ô‡πÅ‡∏ú‡∏ô‡∏á‡∏≤‡∏ô\n",
        "3. **Optimize** ‚Äî ‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ú‡∏ô‡πÉ‡∏´‡πâ‡πÄ‡∏£‡πá‡∏ß‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î (‡∏ú‡πà‡∏≤‡∏ô Catalyst Optimizer)\n",
        "4. **‡πÅ‡∏ö‡πà‡∏á‡∏á‡∏≤‡∏ô** ‚Äî ‡πÅ‡∏ö‡πà‡∏á Job ‚Üí Stages ‚Üí Tasks\n",
        "5. **‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°‡∏ú‡∏•** ‚Äî ‡∏î‡∏π‡∏ß‡πà‡∏≤ Executor ‡∏ó‡∏≥‡πÄ‡∏™‡∏£‡πá‡∏à‡∏´‡∏£‡∏∑‡∏≠‡∏¢‡∏±‡∏á\n",
        "\n",
        "### ‚öôÔ∏è Executor (‡πÅ‡∏£‡∏á‡∏á‡∏≤‡∏ô) ‚Äî ‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà:\n",
        "1. **‡∏£‡∏±‡∏ö Task ‡∏°‡∏≤‡∏ó‡∏≥** ‚Äî ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏£‡∏¥‡∏á ‡πÜ\n",
        "2. **‡πÄ‡∏Å‡πá‡∏ö Cache** ‚Äî ‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô RAM ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÉ‡∏ä‡πâ‡∏ã‡πâ‡∏≥\n",
        "3. **Shuffle** ‚Äî ‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡πâ‡∏≤‡∏°‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô\n",
        "4. **‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏ú‡∏•** ‚Äî ‡∏™‡πà‡∏á‡∏ú‡∏•‡∏Å‡∏•‡∏±‡∏ö‡πÑ‡∏õ‡∏ó‡∏µ‡πà Driver\n",
        "\n",
        "### üîÑ ‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô: Job ‚Üí Stage ‚Üí Task ‚Üí Partition\n",
        "\n",
        "| ‡∏£‡∏∞‡∏î‡∏±‡∏ö | ‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢ | ‡πÄ‡∏Å‡∏¥‡∏î‡∏à‡∏≤‡∏Å |\n",
        "|------|----------|--------|\n",
        "| **Job** | ‡∏á‡∏≤‡∏ô‡πÉ‡∏´‡∏ç‡πà 1 ‡∏ä‡∏¥‡πâ‡∏ô | ‡πÄ‡∏Å‡∏¥‡∏î‡∏à‡∏≤‡∏Å Action (‡πÄ‡∏ä‡πà‡∏ô `.count()`) |\n",
        "| **Stage** | ‡∏ä‡πà‡∏ß‡∏á‡∏á‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡πÑ‡∏î‡πâ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á Shuffle | ‡πÄ‡∏Å‡∏¥‡∏î‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏ó‡∏µ‡πà‡∏à‡∏∏‡∏î Shuffle |\n",
        "| **Task** | ‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô‡∏¢‡πà‡∏≠‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î | 1 Task ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• 1 Partition |\n",
        "| **Partition** | ‡∏ä‡∏¥‡πâ‡∏ô‡∏™‡πà‡∏ß‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• | ‡πÑ‡∏ü‡∏•‡πå‡∏ñ‡∏π‡∏Å‡πÅ‡∏ö‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô Partition ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥ |\n",
        "\n",
        "> üí° **‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö:** Job = ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ö‡πâ‡∏≤‡∏ô, Stage = ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô (‡πÄ‡∏ó‡∏ê‡∏≤‡∏ô/‡∏Å‡πà‡∏≠‡∏ú‡∏ô‡∏±‡∏á/‡∏°‡∏∏‡∏á‡∏´‡∏•‡∏±‡∏á‡∏Ñ‡∏≤), Task = ‡∏ä‡πà‡∏≤‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏ô, Partition = ‡∏ß‡∏±‡∏™‡∏î‡∏∏‡∏ó‡∏µ‡πà‡πÅ‡∏ö‡πà‡∏á‡πÉ‡∏´‡πâ‡∏ä‡πà‡∏≤‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏ô\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "added_md_89bd6e99",
      "metadata": {
        "id": "added_md_89bd6e99"
      },
      "source": [
        "<div align=\"center\">\n",
        "  <img src=\"https://github.com/witsarutsarai12-Academic/128-356-Big-Data/blob/main/images/driver_internal.png?raw=1\" width=\"800\" alt=\"Driver Internals Diagram\">\n",
        "  <br><i>‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏†‡∏≤‡∏¢‡πÉ‡∏ô‡∏Ç‡∏≠‡∏á Driver: SparkSession, DAGScheduler, TaskScheduler</i>\n",
        "</div>\n",
        "\n",
        "#### üß† ‡πÄ‡∏à‡∏≤‡∏∞‡∏•‡∏∂‡∏Å Driver Internals\n",
        "- **SparkSession**: ‡∏à‡∏∏‡∏î‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÇ‡∏Ñ‡πâ‡∏î (Entry Point)\n",
        "- **DAGScheduler**: ‡πÅ‡∏õ‡∏•‡∏á Logical Plan ‡πÄ‡∏õ‡πá‡∏ô Physical Plan ‡πÅ‡∏•‡∏∞‡πÅ‡∏ö‡πà‡∏á‡∏á‡∏≤‡∏ô‡πÄ‡∏õ‡πá‡∏ô Stages (Stage 1, Stage 2)\n",
        "- **TaskScheduler**: ‡∏£‡∏±‡∏ö Stage ‡∏°‡∏≤‡πÅ‡∏ï‡∏Å‡πÄ‡∏õ‡πá‡∏ô Task ‡∏¢‡πà‡∏≠‡∏¢‡πÜ ‡πÅ‡∏•‡πâ‡∏ß‡∏™‡πà‡∏á‡πÑ‡∏õ‡πÉ‡∏´‡πâ Executor ‡∏ó‡∏µ‡πà‡∏ß‡πà‡∏≤‡∏á‡∏≠‡∏¢‡∏π‡πà\n",
        "- **SchedulerBackend**: ‡∏™‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏£‡∏Å‡∏±‡∏ö Cluster Manager ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ç‡∏≠‡∏ó‡∏£‡∏±‡∏û‡∏¢‡∏≤‡∏Å‡∏£ (CPU/RAM)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "img_spark_chef_analogy",
      "metadata": {
        "id": "img_spark_chef_analogy"
      },
      "source": [
        "<div align=\"center\">\n",
        "  <img src=\"https://github.com/witsarutsarai12-Academic/128-356-Big-Data/blob/main/images/spark_chef_analogy.png?raw=1\" width=\"800\" alt=\"Spark Architecture Analogy: Driver (Chef) ‡∏™‡∏±‡πà‡∏á‡∏á‡∏≤‡∏ô Executors (Cooks)\">\n",
        "  <br><i>Spark Architecture Analogy: Driver (Chef) ‡∏™‡∏±‡πà‡∏á‡∏á‡∏≤‡∏ô Executors (Cooks)</i>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell_7",
      "metadata": {
        "id": "cell_7"
      },
      "source": [
        "### 3.1 üß† Deep Dive: SparkSession vs SparkContext\n",
        "\n",
        "‡∏´‡∏•‡∏≤‡∏¢‡∏Ñ‡∏ô‡∏≠‡∏≤‡∏à‡∏™‡∏á‡∏™‡∏±‡∏¢‡∏ß‡πà‡∏≤ `SparkSession` ‡∏Å‡∏±‡∏ö `SparkContext` ‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£?\n",
        "\n",
        "#### 1. SparkContext (`sc`) ‚Äî The Engine üîß\n",
        "- ‡πÄ‡∏õ‡πá‡∏ô **Entry Point** ‡∏î‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏¥‡∏°‡∏Ç‡∏≠‡∏á Spark (‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ï‡πà‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô 1.x)\n",
        "- ‡∏ó‡∏≥‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏Å‡∏±‡∏ö Cluster Manager (YARN, K8s, Standalone)\n",
        "- ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Executors, Memory, ‡πÅ‡∏•‡∏∞ Job Scheduling\n",
        "- **‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö:** ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô **\"‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏¢‡∏ô‡∏ï‡πå\"** ‡∏Ç‡∏≠‡∏á‡∏£‡∏ñ‡∏¢‡∏ô‡∏ï‡πå ‚Äî ‡∏ó‡∏≥‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏Ç‡∏±‡∏ö‡πÄ‡∏Ñ‡∏•‡∏∑‡πà‡∏≠‡∏ô‡∏Å‡∏•‡πÑ‡∏Å‡∏†‡∏≤‡∏¢‡πÉ‡∏ô\n",
        "\n",
        "#### 2. SparkSession (`spark`) ‚Äî The Dashboard üöó\n",
        "- ‡πÄ‡∏£‡∏¥‡πà‡∏°‡πÉ‡∏ä‡πâ‡πÉ‡∏ô Spark 2.0+\n",
        "- ‡πÄ‡∏õ‡πá‡∏ô **Unified Entry Point** ‡∏ó‡∏µ‡πà‡∏£‡∏ß‡∏°‡∏ó‡∏∏‡∏Å‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏ß‡πâ‡πÉ‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏î‡∏µ‡∏¢‡∏ß:\n",
        "  - `SparkContext` (Core)\n",
        "  - `SQLContext` (DataFrames/SQL)\n",
        "  - `HiveContext` (Hive tables)\n",
        "  - `StreamingContext` (Streaming)\n",
        "- **‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö:** ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô **\"‡∏Ñ‡∏ô‡∏Ç‡∏±‡∏ö\"** ‡∏´‡∏£‡∏∑‡∏≠ **\"‡πÅ‡∏ú‡∏á‡∏´‡∏ô‡πâ‡∏≤‡∏õ‡∏±‡∏î\"** ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏à‡∏£‡∏¥‡∏á (‡πÄ‡∏£‡∏≤‡∏Ç‡∏±‡∏ö‡∏£‡∏ñ‡∏ú‡πà‡∏≤‡∏ô‡∏û‡∏ß‡∏á‡∏°‡∏≤‡∏•‡∏±‡∏¢ ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÑ‡∏õ‡∏´‡∏°‡∏∏‡∏ô‡∏•‡πâ‡∏≠‡πÄ‡∏≠‡∏á)\n",
        "\n",
        "> üí° **Best Practice:** ‡πÉ‡∏ô‡πÇ‡∏Ñ‡πâ‡∏î‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô **‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÉ‡∏ä‡πâ `SparkSession` ‡πÄ‡∏™‡∏°‡∏≠** (‡πÅ‡∏ï‡πà‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏´‡∏•‡∏±‡∏á‡∏°‡∏±‡∏ô‡∏Å‡πá‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ `SparkContext` ‡∏≠‡∏¢‡∏π‡πà‡∏î‡∏µ)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell_8",
      "metadata": {
        "id": "cell_8"
      },
      "source": [
        "## üî¨ Lab: ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô Spark\n",
        "\n",
        "### Step 1: ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á SparkSession\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "cell_9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cell_9",
        "outputId": "04869d7e-416c-4810-9f75-cdd7c8ac2be6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Spark Version: 4.0.2\n",
            "üì± App Name: BigData-Week4-DeepDive\n",
            "üñ•Ô∏è Master: local[*]\n"
          ]
        }
      ],
      "source": [
        "# ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á PySpark (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Colab)\n",
        "!pip -q install pyspark\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "import os\n",
        "\n",
        "# ‡∏™‡∏£‡πâ‡∏≤‡∏á SparkSession ‚Äî ‡∏õ‡∏£‡∏∞‡∏ï‡∏π‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏π‡πà Spark\n",
        "# üí° config(\"spark.ui.port\", \"4050\") ‡∏Å‡∏≥‡∏´‡∏ô‡∏î port ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á port 4040 ‡∏ó‡∏µ‡πà‡∏≠‡∏≤‡∏à‡∏ä‡∏ô‡∏Å‡∏±‡∏ô\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"BigData-Week4-DeepDive\") \\\n",
        "    .config(\"spark.driver.memory\", \"2g\") \\\n",
        "    .config(\"spark.ui.port\", \"4050\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# ‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á SparkContext ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å wrap ‡∏≠‡∏¢‡∏π‡πà‡∏Ç‡πâ‡∏≤‡∏á‡πÉ‡∏ô\n",
        "sc = spark.sparkContext\n",
        "\n",
        "print(f\"‚úÖ Spark Version: {spark.version}\")\n",
        "print(f\"üì± App Name: {sc.appName}\")\n",
        "print(f\"üñ•Ô∏è Master: {sc.master}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell_10",
      "metadata": {
        "id": "cell_10"
      },
      "source": [
        "### 3.2 üõ†Ô∏è ‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤: ‡πÄ‡∏õ‡∏¥‡∏î Spark UI ‡πÉ‡∏ô Google Colab\n",
        "\n",
        "**‡∏õ‡∏±‡∏ç‡∏´‡∏≤:** ‡∏õ‡∏Å‡∏ï‡∏¥ Spark UI ‡∏à‡∏∞‡∏≠‡∏¢‡∏π‡πà‡∏ó‡∏µ‡πà `localhost:4040` ‡πÅ‡∏ï‡πà Google Colab ‡∏£‡∏±‡∏ô‡∏ö‡∏ô‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á Server (Virtual Machine) ‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡πÄ‡∏£‡∏≤‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á `localhost` ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á\n",
        "\n",
        "**‡∏ß‡∏¥‡∏ò‡∏µ‡πÅ‡∏Å‡πâ:** ‡πÄ‡∏£‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ library `google.colab.output` ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á Proxy ‡πÉ‡∏´‡πâ‡πÄ‡∏£‡∏≤‡πÄ‡∏õ‡∏¥‡∏î‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏ß‡πá‡∏ö‡πÑ‡∏î‡πâ\n",
        "\n",
        "‡∏£‡∏±‡∏ô cell ‡∏ô‡∏µ‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏õ‡∏¥‡∏î Spark UI üëá\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "cell_11",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 508
        },
        "collapsed": true,
        "id": "cell_11",
        "outputId": "acb8144a-c27c-46d5-aa49-9e969d9fd07a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, width, height, cache, element) => {\n",
              "    if (!google.colab.kernel.accessAllowed && !cache) {\n",
              "      return;\n",
              "    }\n",
              "    element.appendChild(document.createTextNode(''));\n",
              "    const url = await google.colab.kernel.proxyPort(port, {cache});\n",
              "    const iframe = document.createElement('iframe');\n",
              "    iframe.src = new URL(path, url).toString();\n",
              "    iframe.height = height;\n",
              "    iframe.width = width;\n",
              "    iframe.style.border = 0;\n",
              "    iframe.allow = [\n",
              "        'accelerometer',\n",
              "        'autoplay',\n",
              "        'camera',\n",
              "        'clipboard-read',\n",
              "        'clipboard-write',\n",
              "        'gyroscope',\n",
              "        'magnetometer',\n",
              "        'microphone',\n",
              "        'serial',\n",
              "        'usb',\n",
              "        'xr-spatial-tracking',\n",
              "    ].join('; ');\n",
              "    element.appendChild(iframe);\n",
              "  })(4050, \"/\", \"100%\", \"400\", false, window.element)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Spark UI ‡πÄ‡∏õ‡∏¥‡∏î‡∏≠‡∏¢‡∏π‡πà‡∏ó‡∏µ‡πà: None\n",
            "üí° ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏´‡πá‡∏ô‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå ‡πÉ‡∏´‡πâ‡∏•‡∏≠‡∏á‡∏Ñ‡∏•‡∏¥‡∏Å link ‡∏ô‡∏µ‡πâ‡πÅ‡∏ó‡∏ô (‡∏≠‡∏≤‡∏à‡∏ï‡πâ‡∏≠‡∏á login Google):\n",
            "\u001b[31mWarning: This function may stop working due to changes in browser security.\n",
            "Try `serve_kernel_port_as_iframe` instead. \u001b[0m\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, text, element) => {\n",
              "    if (!google.colab.kernel.accessAllowed) {\n",
              "      return;\n",
              "    }\n",
              "    element.appendChild(document.createTextNode(''));\n",
              "    const url = await google.colab.kernel.proxyPort(port);\n",
              "    const anchor = document.createElement('a');\n",
              "    anchor.href = new URL(path, url).toString();\n",
              "    anchor.target = '_blank';\n",
              "    anchor.setAttribute('data-href', url + path);\n",
              "    anchor.textContent = text;\n",
              "    element.appendChild(anchor);\n",
              "  })(4050, \"/jobs/\", \"https://localhost:4050/jobs/\", window.element)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# üõ†Ô∏è Code ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏õ‡∏¥‡∏î Spark UI ‡πÉ‡∏ô Colab\n",
        "try:\n",
        "    from google.colab import output\n",
        "\n",
        "    # ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏õ‡∏¥‡∏î Spark UI\n",
        "    def show_spark_ui(port=4050):\n",
        "        # ‡∏î‡∏∂‡∏á URL ‡∏Ç‡∏≠‡∏á Spark UI ‡∏ú‡πà‡∏≤‡∏ô Proxy ‡∏Ç‡∏≠‡∏á Colab\n",
        "        url = output.serve_kernel_port_as_iframe(port)\n",
        "        # ‡∏´‡∏£‡∏∑‡∏≠‡πÅ‡∏™‡∏î‡∏á‡πÄ‡∏õ‡πá‡∏ô Link ‡πÉ‡∏´‡πâ‡∏Ñ‡∏•‡∏¥‡∏Å (‡∏ñ‡πâ‡∏≤ iframe ‡πÄ‡∏•‡πá‡∏Å‡πÑ‡∏õ)\n",
        "        # output.serve_kernel_port_as_window(port)\n",
        "        print(f\"üöÄ Spark UI ‡πÄ‡∏õ‡∏¥‡∏î‡∏≠‡∏¢‡∏π‡πà‡∏ó‡∏µ‡πà: {url}\")\n",
        "        print(\"üí° ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏´‡πá‡∏ô‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå ‡πÉ‡∏´‡πâ‡∏•‡∏≠‡∏á‡∏Ñ‡∏•‡∏¥‡∏Å link ‡∏ô‡∏µ‡πâ‡πÅ‡∏ó‡∏ô (‡∏≠‡∏≤‡∏à‡∏ï‡πâ‡∏≠‡∏á login Google):\")\n",
        "        output.serve_kernel_port_as_window(port, path='/jobs/')\n",
        "\n",
        "    # ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô (Default port ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏ï‡∏±‡πâ‡∏á‡πÑ‡∏ß‡πâ‡∏Ñ‡∏∑‡∏≠ 4050)\n",
        "    show_spark_ui(4050)\n",
        "except ImportError:\n",
        "    print(\"‚ö†Ô∏è ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ô‡∏ö‡∏ô Google Colab ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà‡∏û‡∏ö library google.colab\")\n",
        "    print(f\"üëâ ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏∏‡∏ì‡∏£‡∏±‡∏ô‡∏ö‡∏ô Local Jupyter ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡∏¥‡∏î: http://localhost:4050\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell_12",
      "metadata": {
        "id": "cell_12"
      },
      "source": [
        "# 4. üè¢ Cluster Managers (‡∏ú‡∏π‡πâ‡∏à‡∏±‡∏î‡∏™‡∏£‡∏£‡∏ó‡∏£‡∏±‡∏û‡∏¢‡∏≤‡∏Å‡∏£)\n",
        "\n",
        "Cluster Manager ‡∏ó‡∏≥‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏à‡∏±‡∏î‡∏™‡∏£‡∏£ CPU ‡πÅ‡∏•‡∏∞ RAM ‡πÉ‡∏´‡πâ Spark ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÑ‡∏î‡πâ‡∏Å‡∏±‡∏ö \"‡∏ú‡∏π‡πâ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏≠‡∏≤‡∏Ñ‡∏≤‡∏£\" ‡∏ó‡∏µ‡πà‡∏à‡∏±‡∏î‡∏´‡πâ‡∏≠‡∏á‡πÉ‡∏´‡πâ‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡πà‡∏≤\n",
        "\n",
        "## ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö Cluster Manager 3 ‡πÅ‡∏ö‡∏ö\n",
        "\n",
        "| | **Standalone** | **YARN** | **Kubernetes** |\n",
        "|---|---|---|---|\n",
        "| **‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢** | Cluster Manager ‡∏Ç‡∏≠‡∏á Spark ‡πÄ‡∏≠‡∏á | Hadoop ecosystem | Container-based |\n",
        "| **‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á** | ‡∏á‡πà‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î | ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ Hadoop | ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ K8s cluster |\n",
        "| **‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö** | ‡∏ó‡∏î‡∏•‡∏≠‡∏á, ‡∏ó‡∏µ‡∏°‡πÄ‡∏•‡πá‡∏Å | ‡∏≠‡∏á‡∏Ñ‡πå‡∏Å‡∏£‡πÉ‡∏´‡∏ç‡πà | Cloud-native |\n",
        "| **‡∏Ç‡πâ‡∏≠‡∏î‡∏µ** | Simple, ‡πÄ‡∏£‡πá‡∏ß | Queue management, ‡πÅ‡∏ä‡∏£‡πå resource | Auto-scaling, Isolate |\n",
        "| **‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á** | Spark Standalone | AWS EMR, CDH | GKE, EKS |\n",
        "\n",
        "> üí° **‡πÉ‡∏ô‡∏Ñ‡∏≠‡∏£‡πå‡∏™‡∏ô‡∏µ‡πâ** ‡πÄ‡∏£‡∏≤‡πÉ‡∏ä‡πâ `local[*]` ‡∏ã‡∏∂‡πà‡∏á‡∏´‡∏°‡∏≤‡∏¢‡∏ñ‡∏∂‡∏á \"‡∏£‡∏±‡∏ô‡∏ö‡∏ô‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß ‡πÉ‡∏ä‡πâ‡∏ó‡∏∏‡∏Å CPU core\" ‚Äî ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "cell_13",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cell_13",
        "outputId": "a540ff39-f30a-4ad0-f78b-5145fd0515dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üè¢ Master URL: local[*]\n",
            "\n",
            "üìù ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á Master URL:\n",
            "  local     = ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß, 1 thread\n",
            "  local[*]  = ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß, ‡πÉ‡∏ä‡πâ‡∏ó‡∏∏‡∏Å core\n",
            "  local[4]  = ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß, 4 threads\n",
            "  yarn      = ‡πÉ‡∏ä‡πâ YARN ‡∏ö‡∏ô Hadoop cluster\n",
            "  k8s://... = ‡πÉ‡∏ä‡πâ Kubernetes cluster\n",
            "  spark://  = ‡πÉ‡∏ä‡πâ Standalone cluster\n"
          ]
        }
      ],
      "source": [
        "# ‡∏î‡∏π‡∏ß‡πà‡∏≤‡πÄ‡∏£‡∏≤‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÉ‡∏ä‡πâ Cluster Manager ‡πÅ‡∏ö‡∏ö‡πÑ‡∏´‡∏ô\n",
        "print(f\"üè¢ Master URL: {spark.sparkContext.master}\")\n",
        "print()\n",
        "print(\"üìù ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á Master URL:\")\n",
        "print(\"  local     = ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß, 1 thread\")\n",
        "print(\"  local[*]  = ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß, ‡πÉ‡∏ä‡πâ‡∏ó‡∏∏‡∏Å core\")\n",
        "print(\"  local[4]  = ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß, 4 threads\")\n",
        "print(\"  yarn      = ‡πÉ‡∏ä‡πâ YARN ‡∏ö‡∏ô Hadoop cluster\")\n",
        "print(\"  k8s://... = ‡πÉ‡∏ä‡πâ Kubernetes cluster\")\n",
        "print(\"  spark://  = ‡πÉ‡∏ä‡πâ Standalone cluster\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell_14",
      "metadata": {
        "id": "cell_14"
      },
      "source": [
        "# 5. ‚è≥ Lazy Evaluation (‡πÅ‡∏ô‡∏ß‡∏Ñ‡∏¥‡∏î‡πÄ‡∏ä‡∏¥‡∏á‡∏•‡∏∂‡∏Å)\n",
        "\n",
        "## ‡∏´‡∏•‡∏±‡∏Å‡∏Å‡∏≤‡∏£‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏Ç‡∏≠‡∏á Spark\n",
        "\n",
        "> **Spark ‡πÑ‡∏°‡πà‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏ó‡∏±‡∏ô‡∏ó‡∏µ!** Spark ‡∏à‡∏∞ \"‡∏à‡∏î‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å\" ‡∏ß‡πà‡∏≤‡πÄ‡∏£‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏≠‡∏∞‡πÑ‡∏£ ‡πÅ‡∏•‡πâ‡∏ß‡∏Ñ‡πà‡∏≠‡∏¢‡∏ó‡∏≥‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏à‡∏£‡∏¥‡∏á ‡πÜ\n",
        "\n",
        "### ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö\n",
        "\n",
        "| | Pandas (Eager) | Spark (Lazy) |\n",
        "|---|---|---|\n",
        "| **‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á** | ‡∏ó‡∏≥‡∏ó‡∏±‡∏ô‡∏ó‡∏µ | ‡∏à‡∏î‡πÑ‡∏ß‡πâ‡∏Å‡πà‡∏≠‡∏ô (‡∏™‡∏£‡πâ‡∏≤‡∏á Plan) |\n",
        "| **‡∏Ç‡πâ‡∏≠‡∏î‡∏µ** | ‡πÄ‡∏´‡πá‡∏ô‡∏ú‡∏•‡πÄ‡∏•‡∏¢ | ‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ú‡∏ô‡πÉ‡∏´‡πâ‡πÄ‡∏£‡πá‡∏ß‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏Å‡πà‡∏≠‡∏ô‡∏ó‡∏≥ |\n",
        "| **‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏µ‡∏¢** | ‡πÑ‡∏°‡πà optimize ‡∏Ç‡πâ‡∏≤‡∏°‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô | ‡∏ï‡πâ‡∏≠‡∏á trigger ‡∏î‡πâ‡∏ß‡∏¢ Action |\n",
        "\n",
        "### Transformation vs Action\n",
        "\n",
        "#### üîπ Transformations (Lazy ‚Äî ‡∏à‡∏î‡πÑ‡∏ß‡πâ‡∏Å‡πà‡∏≠‡∏ô ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏ó‡∏≥)\n",
        "| ‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á | ‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà |\n",
        "|--------|--------|\n",
        "| `select()` | ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå |\n",
        "| `filter()` / `where()` | ‡∏Å‡∏£‡∏≠‡∏á‡πÅ‡∏ñ‡∏ß |\n",
        "| `groupBy()` | ‡∏à‡∏±‡∏î‡∏Å‡∏•‡∏∏‡πà‡∏° |\n",
        "| `join()` | ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡∏≤‡∏£‡∏≤‡∏á |\n",
        "| `withColumn()` | ‡πÄ‡∏û‡∏¥‡πà‡∏°/‡πÅ‡∏Å‡πâ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå |\n",
        "| `orderBy()` | ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö |\n",
        "\n",
        "#### üî∏ Actions (‡∏ï‡∏±‡∏ß‡∏Å‡∏£‡∏∞‡∏ï‡∏∏‡πâ‡∏ô ‚Äî ‡∏™‡∏±‡πà‡∏á‡πÉ‡∏´‡πâ‡∏ó‡∏≥‡∏à‡∏£‡∏¥‡∏á!)\n",
        "| ‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á | ‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà |\n",
        "|--------|--------|\n",
        "| `show()` | ‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• |\n",
        "| `count()` | ‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÅ‡∏ñ‡∏ß |\n",
        "| `collect()` | ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏°‡∏≤‡∏ó‡∏µ‡πà Driver |\n",
        "| `write()` | ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏•‡∏á disk |\n",
        "| `take(n)` | ‡∏î‡∏∂‡∏á n ‡πÅ‡∏ñ‡∏ß‡πÅ‡∏£‡∏Å |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "img_lazy_evaluation",
      "metadata": {
        "id": "img_lazy_evaluation"
      },
      "source": [
        "<div align=\"center\">\n",
        "  <img src=\"https://github.com/witsarutsarai12-Academic/128-356-Big-Data/blob/main/images/lazy_evaluation.png?raw=1\" width=\"800\" alt=\"Lazy Evaluation: ‡∏à‡∏î‡∏≠‡∏≠‡∏£‡πå‡πÄ‡∏î‡∏≠‡∏£‡πå (Transformation) vs ‡πÄ‡∏™‡∏¥‡∏£‡πå‡∏ü‡∏≠‡∏≤‡∏´‡∏≤‡∏£ (Action)\">\n",
        "  <br><i>Lazy Evaluation: ‡∏à‡∏î‡∏≠‡∏≠‡∏£‡πå‡πÄ‡∏î‡∏≠‡∏£‡πå (Transformation) vs ‡πÄ‡∏™‡∏¥‡∏£‡πå‡∏ü‡∏≠‡∏≤‡∏´‡∏≤‡∏£ (Action)</i>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell_15",
      "metadata": {
        "id": "cell_15"
      },
      "source": [
        "### üî¨ Lab: ‡∏û‡∏¥‡∏™‡∏π‡∏à‡∏ô‡πå Lazy Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "cell_16",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "cell_16",
        "outputId": "6da5bfb4-401f-4c4e-f17e-4a8b21192e3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á DataFrame ‡∏Ç‡∏ô‡∏≤‡∏î 10000 ‡πÅ‡∏ñ‡∏ß\n",
            "+---+------+------+----+\n",
            "| id|  name|salary|dept|\n",
            "+---+------+------+----+\n",
            "|  1|name_1|   100|   B|\n",
            "|  2|name_2|   200|   A|\n",
            "|  3|name_3|   300|   B|\n",
            "|  4|name_4|   400|   A|\n",
            "|  5|name_5|   500|   B|\n",
            "+---+------+------+----+\n",
            "only showing top 5 rows\n"
          ]
        }
      ],
      "source": [
        "# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á\n",
        "data = [(i, f\"name_{i}\", i * 100, \"A\" if i % 2 == 0 else \"B\")\n",
        "        for i in range(1, 10001)]\n",
        "df = spark.createDataFrame(data, [\"id\", \"name\", \"salary\", \"dept\"])\n",
        "\n",
        "print(f\"‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á DataFrame ‡∏Ç‡∏ô‡∏≤‡∏î {df.count()} ‡πÅ‡∏ñ‡∏ß\")\n",
        "df.show(5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "cell_17",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "cell_17",
        "outputId": "0c4c27e5-1b52-4f2c-ad5d-e17cab8eac1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîπ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô Transformation...\n",
            "‚è±Ô∏è ‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ (Transformation): 0.1538 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ\n",
            "üí° ‡∏™‡∏±‡∏á‡πÄ‡∏Å‡∏ï: ‡πÄ‡∏£‡πá‡∏ß‡∏°‡∏≤‡∏Å ‡πÄ‡∏û‡∏£‡∏≤‡∏∞ Spark ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ó‡∏≥‡∏≠‡∏∞‡πÑ‡∏£‡πÄ‡∏•‡∏¢!\n",
            "\n",
            "üî∏ ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å Action (.show()) ‚Äî Spark ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ó‡∏≥‡∏á‡∏≤‡∏ô!\n",
            "+----+-----+\n",
            "|dept|count|\n",
            "+----+-----+\n",
            "|   B| 4997|\n",
            "|   A| 4998|\n",
            "+----+-----+\n",
            "\n",
            "‚è±Ô∏è ‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ (Action): 1.9786 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ\n",
            "üí° ‡∏™‡∏±‡∏á‡πÄ‡∏Å‡∏ï: ‡∏ä‡πâ‡∏≤‡∏Å‡∏ß‡πà‡∏≤ ‡πÄ‡∏û‡∏£‡∏≤‡∏∞ Spark ‡∏ï‡πâ‡∏≠‡∏á‡∏ó‡∏≥ Transformation ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ!\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "# ===== Transformation (Lazy) ‚Äî ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏≠‡∏∞‡πÑ‡∏£‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡∏∂‡πâ‡∏ô! =====\n",
        "print(\"üîπ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô Transformation...\")\n",
        "t0 = time.time()\n",
        "\n",
        "df2 = df.select(\"id\", \"salary\", \"dept\")      # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå\n",
        "df3 = df2.filter(df2[\"salary\"] > 500)         # ‡∏Å‡∏£‡∏≠‡∏á\n",
        "df4 = df3.groupBy(\"dept\").count()             # ‡∏à‡∏±‡∏î‡∏Å‡∏•‡∏∏‡πà‡∏°\n",
        "\n",
        "t1 = time.time()\n",
        "print(f\"‚è±Ô∏è ‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ (Transformation): {t1-t0:.4f} ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ\")\n",
        "print(\"üí° ‡∏™‡∏±‡∏á‡πÄ‡∏Å‡∏ï: ‡πÄ‡∏£‡πá‡∏ß‡∏°‡∏≤‡∏Å ‡πÄ‡∏û‡∏£‡∏≤‡∏∞ Spark ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ó‡∏≥‡∏≠‡∏∞‡πÑ‡∏£‡πÄ‡∏•‡∏¢!\")\n",
        "\n",
        "# ===== Action (Trigger!) ‚Äî ‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ Spark ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏à‡∏£‡∏¥‡∏á! =====\n",
        "print(\"\\nüî∏ ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å Action (.show()) ‚Äî Spark ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ó‡∏≥‡∏á‡∏≤‡∏ô!\")\n",
        "t2 = time.time()\n",
        "\n",
        "df4.show()\n",
        "\n",
        "t3 = time.time()\n",
        "print(f\"‚è±Ô∏è ‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ (Action): {t3-t2:.4f} ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ\")\n",
        "print(\"üí° ‡∏™‡∏±‡∏á‡πÄ‡∏Å‡∏ï: ‡∏ä‡πâ‡∏≤‡∏Å‡∏ß‡πà‡∏≤ ‡πÄ‡∏û‡∏£‡∏≤‡∏∞ Spark ‡∏ï‡πâ‡∏≠‡∏á‡∏ó‡∏≥ Transformation ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell_18",
      "metadata": {
        "id": "cell_18"
      },
      "source": [
        "### ‚úèÔ∏è ‡πÅ‡∏ö‡∏ö‡∏ù‡∏∂‡∏Å‡∏´‡∏±‡∏î‡∏ó‡∏µ‡πà 2: Transformation vs Action\n",
        "\n",
        "**‡πÇ‡∏à‡∏ó‡∏¢‡πå:** ‡∏à‡∏≤‡∏Å‡πÇ‡∏Ñ‡πâ‡∏î‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á ‡∏£‡∏∞‡∏ö‡∏∏‡∏ß‡πà‡∏≤‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡πÑ‡∏´‡∏ô‡πÄ‡∏õ‡πá‡∏ô Transformation (T) ‡∏´‡∏£‡∏∑‡∏≠ Action (A)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "cell_19",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "cell_19",
        "outputId": "cd4bceee-7940-47c6-99b6-7b24559b1da9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ó‡∏µ‡πà 1: T\n",
            "‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ó‡∏µ‡πà 2: t\n",
            "‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ó‡∏µ‡πà 3: a\n",
            "‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ó‡∏µ‡πà 4: t\n",
            "‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ó‡∏µ‡πà 5: a\n",
            "‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ó‡∏µ‡πà 6: t\n",
            "‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ó‡∏µ‡πà 7: a\n",
            "‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ó‡∏µ‡πà 8: a\n"
          ]
        }
      ],
      "source": [
        "# ‡πÅ‡∏ö‡∏ö‡∏ù‡∏∂‡∏Å‡∏´‡∏±‡∏î‡∏ó‡∏µ‡πà 2: ‡πÄ‡∏ï‡∏¥‡∏° T (Transformation) ‡∏´‡∏£‡∏∑‡∏≠ A (Action)\n",
        "\n",
        "# ‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ó‡∏µ‡πà 1: df.select(\"name\", \"salary\")      ‚Üí  ________\n",
        "# ‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ó‡∏µ‡πà 2: df.filter(df[\"salary\"] > 1000)    ‚Üí  ________\n",
        "# ‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ó‡∏µ‡πà 3: df.count()                        ‚Üí  ________\n",
        "# ‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ó‡∏µ‡πà 4: df.groupBy(\"dept\").avg(\"salary\")  ‚Üí  ________\n",
        "# ‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ó‡∏µ‡πà 5: df.show(10)                       ‚Üí  ________\n",
        "# ‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ó‡∏µ‡πà 6: df.orderBy(\"salary\")              ‚Üí  ________\n",
        "# ‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ó‡∏µ‡πà 7: df.collect()                      ‚Üí  ________\n",
        "# ‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ó‡∏µ‡πà 8: df.withColumn(\"bonus\", df[\"salary\"] * 0.1)  ‚Üí  ________\n",
        "\n",
        "answers = {\n",
        "    1: \"T\",\n",
        "    2: \"t\",\n",
        "    3: \"a\",\n",
        "    4: \"t\",\n",
        "    5: \"a\",\n",
        "    6: \"t\",\n",
        "    7: \"a\",\n",
        "    8: \"a\",\n",
        "}\n",
        "\n",
        "for k, v in answers.items():\n",
        "    print(f\"‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ó‡∏µ‡πà {k}: {v}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell_20",
      "metadata": {
        "id": "cell_20"
      },
      "source": [
        "# 6. üìä DAG (Directed Acyclic Graph)\n",
        "\n",
        "## DAG ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£?\n",
        "\n",
        "DAG ‡∏Ñ‡∏∑‡∏≠ **‡∏Å‡∏£‡∏≤‡∏ü** ‡πÅ‡∏™‡∏î‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á Spark ‡πÇ‡∏î‡∏¢:\n",
        "- **Directed** = ‡∏°‡∏µ‡∏ó‡∏¥‡∏®‡∏ó‡∏≤‡∏á (‡∏ó‡∏≥‡∏à‡∏≤‡∏Å‡∏ã‡πâ‡∏≤‡∏¢‡πÑ‡∏õ‡∏Ç‡∏ß‡∏≤)\n",
        "- **Acyclic** = ‡πÑ‡∏°‡πà‡∏ß‡∏ô‡∏ã‡πâ‡∏≥ (‡πÑ‡∏°‡πà‡∏Å‡∏•‡∏±‡∏ö‡πÑ‡∏õ‡∏ó‡∏≥‡∏ã‡πâ‡∏≥)\n",
        "- **Graph** = ‡πÅ‡∏ú‡∏ô‡∏†‡∏≤‡∏û‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡πÇ‡∏¢‡∏á\n",
        "\n",
        "```\n",
        "Read CSV ‚Üí Filter ‚Üí Select ‚Üí GroupBy ‚Üí [Shuffle] ‚Üí Aggregate ‚Üí Show\n",
        "  Stage 1 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂  Stage 2 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂\n",
        "```\n",
        "\n",
        "## ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏ú‡∏ô\n",
        "\n",
        "1. **Logical Plan** ‚Äî ‡πÅ‡∏õ‡∏•‡∏á‡∏à‡∏≤‡∏Å‡πÇ‡∏Ñ‡πâ‡∏î‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô\n",
        "2. **Optimized Logical Plan** ‚Äî Catalyst Optimizer ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÉ‡∏´‡πâ‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô\n",
        "3. **Physical Plan** ‚Äî ‡πÅ‡∏ú‡∏ô‡∏à‡∏£‡∏¥‡∏á‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏ó‡∏≥ (‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏≠‡∏±‡∏•‡∏Å‡∏≠‡∏£‡∏¥‡∏ó‡∏∂‡∏°‡∏ó‡∏µ‡πà‡πÄ‡∏£‡πá‡∏ß‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î)\n",
        "\n",
        "> üí° ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö: ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô GPS ‚Äî ‡πÄ‡∏£‡∏≤‡∏ö‡∏≠‡∏Å‡∏à‡∏∏‡∏î‡∏´‡∏°‡∏≤‡∏¢ (Logical) ‚Üí GPS ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÄ‡∏™‡πâ‡∏ô‡∏ó‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î (Optimized) ‚Üí ‡πÅ‡∏™‡∏î‡∏á‡πÄ‡∏™‡πâ‡∏ô‡∏ó‡∏≤‡∏á‡∏à‡∏£‡∏¥‡∏á (Physical)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "cell_21",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "cell_21",
        "outputId": "24b41178-ae5e-4b67-ff2c-1bb59dae35be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìã Execution Plan:\n",
            "==================================================\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan (7)\n",
            "+- HashAggregate (6)\n",
            "   +- Exchange (5)\n",
            "      +- HashAggregate (4)\n",
            "         +- Project (3)\n",
            "            +- Filter (2)\n",
            "               +- Scan ExistingRDD (1)\n",
            "\n",
            "\n",
            "(1) Scan ExistingRDD\n",
            "Output [4]: [id#0L, name#1, salary#2L, dept#3]\n",
            "Arguments: [id#0L, name#1, salary#2L, dept#3], MapPartitionsRDD[4] at applySchemaToPythonRDD at NativeMethodAccessorImpl.java:0, ExistingRDD, UnknownPartitioning(0)\n",
            "\n",
            "(2) Filter\n",
            "Input [4]: [id#0L, name#1, salary#2L, dept#3]\n",
            "Condition : (isnotnull(salary#2L) AND (salary#2L > 500))\n",
            "\n",
            "(3) Project\n",
            "Output [2]: [salary#2L, dept#3]\n",
            "Input [4]: [id#0L, name#1, salary#2L, dept#3]\n",
            "\n",
            "(4) HashAggregate\n",
            "Input [2]: [salary#2L, dept#3]\n",
            "Keys [1]: [dept#3]\n",
            "Functions [1]: [partial_avg(salary#2L)]\n",
            "Aggregate Attributes [2]: [sum#44, count#45L]\n",
            "Results [3]: [dept#3, sum#46, count#47L]\n",
            "\n",
            "(5) Exchange\n",
            "Input [3]: [dept#3, sum#46, count#47L]\n",
            "Arguments: hashpartitioning(dept#3, 200), ENSURE_REQUIREMENTS, [plan_id=115]\n",
            "\n",
            "(6) HashAggregate\n",
            "Input [3]: [dept#3, sum#46, count#47L]\n",
            "Keys [1]: [dept#3]\n",
            "Functions [1]: [avg(salary#2L)]\n",
            "Aggregate Attributes [1]: [avg(salary#2L)#42]\n",
            "Results [2]: [dept#3, avg(salary#2L)#42 AS avg(salary)#43]\n",
            "\n",
            "(7) AdaptiveSparkPlan\n",
            "Output [2]: [dept#3, avg(salary)#43]\n",
            "Arguments: isFinalPlan=false\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ‡∏î‡∏π Execution Plan ‡∏Ç‡∏≠‡∏á Spark\n",
        "df_plan = df.select(\"id\", \"salary\", \"dept\") \\\n",
        "            .filter(df[\"salary\"] > 500) \\\n",
        "            .groupBy(\"dept\") \\\n",
        "            .avg(\"salary\")\n",
        "\n",
        "print(\"üìã Execution Plan:\")\n",
        "print(\"=\" * 50)\n",
        "df_plan.explain(\"formatted\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "cell_22",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "cell_22",
        "outputId": "a1cb6fa1-9e30-4f8f-ac7e-4724bac02357"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìã Extended Plan (‡∏î‡∏π Logical ‚Üí Physical):\n",
            "==================================================\n",
            "== Parsed Logical Plan ==\n",
            "'Aggregate ['dept], ['dept, unresolvedalias('avg(salary#2L))]\n",
            "+- Filter (salary#2L > cast(500 as bigint))\n",
            "   +- Project [id#0L, salary#2L, dept#3]\n",
            "      +- LogicalRDD [id#0L, name#1, salary#2L, dept#3], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "dept: string, avg(salary): double\n",
            "Aggregate [dept#3], [dept#3, avg(salary#2L) AS avg(salary)#43]\n",
            "+- Filter (salary#2L > cast(500 as bigint))\n",
            "   +- Project [id#0L, salary#2L, dept#3]\n",
            "      +- LogicalRDD [id#0L, name#1, salary#2L, dept#3], false\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Aggregate [dept#3], [dept#3, avg(salary#2L) AS avg(salary)#43]\n",
            "+- Project [salary#2L, dept#3]\n",
            "   +- Filter (isnotnull(salary#2L) AND (salary#2L > 500))\n",
            "      +- LogicalRDD [id#0L, name#1, salary#2L, dept#3], false\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- HashAggregate(keys=[dept#3], functions=[avg(salary#2L)], output=[dept#3, avg(salary)#43])\n",
            "   +- Exchange hashpartitioning(dept#3, 200), ENSURE_REQUIREMENTS, [plan_id=115]\n",
            "      +- HashAggregate(keys=[dept#3], functions=[partial_avg(salary#2L)], output=[dept#3, sum#46, count#47L])\n",
            "         +- Project [salary#2L, dept#3]\n",
            "            +- Filter (isnotnull(salary#2L) AND (salary#2L > 500))\n",
            "               +- Scan ExistingRDD[id#0L,name#1,salary#2L,dept#3]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö: ‡∏î‡∏π‡πÅ‡∏ú‡∏ô‡πÅ‡∏ö‡∏ö‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î (extended)\n",
        "print(\"üìã Extended Plan (‡∏î‡∏π Logical ‚Üí Physical):\")\n",
        "print(\"=\" * 50)\n",
        "df_plan.explain(\"extended\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell_23",
      "metadata": {
        "id": "cell_23"
      },
      "source": [
        "# 7. üîÄ Shuffle ‚Äî ‡∏Å‡∏≤‡∏£‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡πâ‡∏≤‡∏°‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á\n",
        "\n",
        "## Shuffle ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£?\n",
        "\n",
        "Shuffle ‡∏Ñ‡∏∑‡∏≠‡∏Å‡∏≤‡∏£ **‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡πâ‡∏≤‡∏°‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á** (‡∏´‡∏£‡∏∑‡∏≠‡∏Ç‡πâ‡∏≤‡∏° partition) ‡πÄ‡∏°‡∏∑‡πà‡∏≠ Spark ‡∏ï‡πâ‡∏≠‡∏á‡∏à‡∏±‡∏î‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà\n",
        "\n",
        "### ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÑ‡∏´‡∏£‡πà‡πÄ‡∏Å‡∏¥‡∏î Shuffle?\n",
        "\n",
        "| ‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á | ‡∏ó‡∏≥‡πÑ‡∏°‡∏ï‡πâ‡∏≠‡∏á Shuffle | ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á |\n",
        "|--------|-----------------|---------|\n",
        "| `groupBy()` | ‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏•‡∏∏‡πà‡∏°‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô‡∏°‡∏≤‡πÑ‡∏ß‡πâ‡∏ó‡∏µ‡πà‡πÄ‡∏î‡∏µ‡∏¢‡∏ß | ‡∏ô‡∏±‡∏ö‡∏¢‡∏≠‡∏î‡∏Ç‡∏≤‡∏¢‡∏ï‡πà‡∏≠‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î |\n",
        "| `join()` | ‡∏ï‡πâ‡∏≠‡∏á‡∏à‡∏±‡∏ö‡∏Ñ‡∏π‡πà‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å 2 ‡∏ï‡∏≤‡∏£‡∏≤‡∏á | JOIN orders ‡∏Å‡∏±‡∏ö customers |\n",
        "| `orderBy()` | ‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î | ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏ï‡∏≤‡∏°‡∏£‡∏≤‡∏Ñ‡∏≤‡∏à‡∏≤‡∏Å‡∏°‡∏≤‡∏Å‡πÑ‡∏õ‡∏ô‡πâ‡∏≠‡∏¢ |\n",
        "| `distinct()` | ‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏ä‡πá‡∏Ñ‡∏Ñ‡πà‡∏≤‡∏ã‡πâ‡∏≥‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î | ‡∏´‡∏≤‡∏à‡∏≥‡∏ô‡∏ß‡∏ô user ‡πÑ‡∏°‡πà‡∏ã‡πâ‡∏≥ |\n",
        "\n",
        "### ‡∏ó‡∏≥‡πÑ‡∏° Shuffle ‡∏ñ‡∏∂‡∏á \"‡πÅ‡∏û‡∏á\"?\n",
        "\n",
        "> ‚ö†Ô∏è **Shuffle = ‡∏Å‡∏≤‡∏£‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ú‡πà‡∏≤‡∏ô‡πÄ‡∏Ñ‡∏£‡∏∑‡∏≠‡∏Ç‡πà‡∏≤‡∏¢ = ‡∏ä‡πâ‡∏≤!**\n",
        "\n",
        "```\n",
        "‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á 1: [A1, B2, A3]  ‚îÄ‚îÄ‚îÄ‚îÄ Network ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂  ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á 1: [A1, A3, A5] (‡∏Å‡∏•‡∏∏‡πà‡∏° A)\n",
        "‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á 2: [B4, A5, B6]  ‚îÄ‚îÄ‚îÄ‚îÄ Network ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂  ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á 2: [B2, B4, B6] (‡∏Å‡∏•‡∏∏‡πà‡∏° B)\n",
        "```\n",
        "\n",
        "- ‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô disk ‡∏Å‡πà‡∏≠‡∏ô‡∏™‡πà‡∏á (Spill)\n",
        "- ‡∏™‡πà‡∏á‡∏ú‡πà‡∏≤‡∏ô Network (‡∏ä‡πâ‡∏≤‡∏Å‡∏ß‡πà‡∏≤ RAM 1000 ‡πÄ‡∏ó‡πà‡∏≤)\n",
        "- ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏≠‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ñ‡∏£‡∏ö‡∏Å‡πà‡∏≠‡∏ô‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏ï‡πà‡∏≠\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "img_shuffle_concept",
      "metadata": {
        "id": "img_shuffle_concept"
      },
      "source": [
        "<div align=\"center\">\n",
        "  <img src=\"https://github.com/witsarutsarai12-Academic/128-356-Big-Data/blob/main/images/shuffle_concept.png?raw=1\" width=\"800\" alt=\"Shuffle Concept: ‡∏Å‡∏≤‡∏£‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡πâ‡∏≤‡∏°‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡πÅ‡∏¢‡∏Å‡∏™‡∏µ‡∏•‡∏π‡∏Å‡∏ö‡∏≠‡∏•‡∏•‡∏á‡∏ñ‡∏±‡∏á‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á\">\n",
        "  <br><i>Shuffle Concept: ‡∏Å‡∏≤‡∏£‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡πâ‡∏≤‡∏°‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡πÅ‡∏¢‡∏Å‡∏™‡∏µ‡∏•‡∏π‡∏Å‡∏ö‡∏≠‡∏•‡∏•‡∏á‡∏ñ‡∏±‡∏á‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á</i>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "cell_24",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cell_24",
        "outputId": "534a1d78-69f1-4d2b-9c7d-289a1f61f81c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üü¢ Filter + Select (‡πÑ‡∏°‡πà‡∏°‡∏µ Shuffle): 1.139 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ\n",
            "üî¥ GroupBy + Sum (‡∏°‡∏µ Shuffle): 1.758 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ\n",
            "\n",
            "üí° ‡∏™‡∏±‡∏á‡πÄ‡∏Å‡∏ï: GroupBy ‡∏ä‡πâ‡∏≤‡∏Å‡∏ß‡πà‡∏≤ Filter ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏ï‡πâ‡∏≠‡∏á Shuffle!\n"
          ]
        }
      ],
      "source": [
        "# ‡∏™‡∏≤‡∏ò‡∏¥‡∏ï‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á: ‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏ó‡∏µ‡πà‡∏°‡∏µ Shuffle vs ‡πÑ‡∏°‡πà‡∏°‡∏µ Shuffle\n",
        "import time\n",
        "\n",
        "# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà‡∏Ç‡∏∂‡πâ‡∏ô\n",
        "big_data = [(i, f\"dept_{i % 50}\", i * 10.5) for i in range(100000)]\n",
        "big_df = spark.createDataFrame(big_data, [\"id\", \"department\", \"amount\"])\n",
        "\n",
        "# ===== ‡πÑ‡∏°‡πà‡∏°‡∏µ Shuffle (Narrow Transformation) =====\n",
        "t0 = time.time()\n",
        "result1 = big_df.filter(big_df[\"amount\"] > 5000).select(\"id\", \"amount\")\n",
        "result1.count()  # trigger action\n",
        "t1 = time.time()\n",
        "print(f\"üü¢ Filter + Select (‡πÑ‡∏°‡πà‡∏°‡∏µ Shuffle): {t1-t0:.3f} ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ\")\n",
        "\n",
        "# ===== ‡∏°‡∏µ Shuffle (Wide Transformation) =====\n",
        "t2 = time.time()\n",
        "result2 = big_df.groupBy(\"department\").sum(\"amount\")\n",
        "result2.count()  # trigger action\n",
        "t3 = time.time()\n",
        "print(f\"üî¥ GroupBy + Sum (‡∏°‡∏µ Shuffle): {t3-t2:.3f} ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ\")\n",
        "print(f\"\\nüí° ‡∏™‡∏±‡∏á‡πÄ‡∏Å‡∏ï: GroupBy ‡∏ä‡πâ‡∏≤‡∏Å‡∏ß‡πà‡∏≤ Filter ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏ï‡πâ‡∏≠‡∏á Shuffle!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell_25",
      "metadata": {
        "id": "cell_25"
      },
      "source": [
        "# 8. üß™ Lab: ‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡∏∞‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏£‡∏¥‡∏á‡∏î‡πâ‡∏ß‡∏¢ Spark\n",
        "\n",
        "### Step 1: ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• CSV\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "cell_load_data",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "cell_load_data",
        "outputId": "b0337386-f45c-479e-f447-1434ae96086e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2026-02-14 07:08:17--  https://data.austintexas.gov/api/views/fdj4-gpfu/rows.csv?accessType=DOWNLOAD\n",
            "Resolving data.austintexas.gov (data.austintexas.gov)... 52.206.68.26, 52.206.140.205, 52.206.140.199\n",
            "Connecting to data.austintexas.gov (data.austintexas.gov)|52.206.68.26|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/csv]\n",
            "Saving to: ‚Äòcrime.csv‚Äô\n",
            "\n",
            "crime.csv               [ <=>                ] 408.91M   692KB/s    in 15m 29s \n",
            "\n",
            "2026-02-14 07:23:49 (451 KB/s) - ‚Äòcrime.csv‚Äô saved [428768699]\n",
            "\n",
            "‚úÖ Downloaded crime.csv\n",
            "‚úÖ Loaded Data: 2,619,292 rows\n",
            "\n",
            "üìä Original Schema:\n",
            "root\n",
            " |-- Incident Number: long (nullable = true)\n",
            " |-- Highest Offense Description: string (nullable = true)\n",
            " |-- Highest Offense Code: integer (nullable = true)\n",
            " |-- Family Violence: string (nullable = true)\n",
            " |-- Occurred Date Time: string (nullable = true)\n",
            " |-- Occurred Date: string (nullable = true)\n",
            " |-- Occurred Time: integer (nullable = true)\n",
            " |-- Report Date Time: string (nullable = true)\n",
            " |-- Report Date: string (nullable = true)\n",
            " |-- Report Time: integer (nullable = true)\n",
            " |-- Location Type: string (nullable = true)\n",
            " |-- Council District: integer (nullable = true)\n",
            " |-- APD Sector: string (nullable = true)\n",
            " |-- APD District: string (nullable = true)\n",
            " |-- Clearance Status: string (nullable = true)\n",
            " |-- Clearance Date: string (nullable = true)\n",
            " |-- UCR Category: string (nullable = true)\n",
            " |-- Category Description: string (nullable = true)\n",
            " |-- Census Block Group: long (nullable = true)\n",
            "\n",
            "\n",
            "üìã Sample Data:\n",
            "+---------------+---------------------------+--------------------+---------------+------------------+-------------+-------------+-----------------+-----------+-----------+---------------------------------------+----------------+----------+------------+----------------+--------------+------------+--------------------+------------------+\n",
            "|Incident Number|Highest Offense Description|Highest Offense Code|Family Violence|Occurred Date Time|Occurred Date|Occurred Time|Report Date Time |Report Date|Report Time|Location Type                          |Council District|APD Sector|APD District|Clearance Status|Clearance Date|UCR Category|Category Description|Census Block Group|\n",
            "+---------------+---------------------------+--------------------+---------------+------------------+-------------+-------------+-----------------+-----------+-----------+---------------------------------------+----------------+----------+------------+----------------+--------------+------------+--------------------+------------------+\n",
            "|20112440318    |THEFT BY SHOPLIFTING       |607                 |N              |09/01/2011  04:55 |09/01/2011   |455          |09/01/2011  04:55|09/01/2011 |455        |GROCERY / SUPERMARKET                  |5               |FR        |2           |N               |09/19/2011    |23C         |Theft               |4530024371        |\n",
            "|20112501319    |BURGLARY NON RESIDENCE     |502                 |N              |09/02/2011  03:02 |09/02/2011   |302          |09/07/2011  15:17|09/07/2011 |1517       |COMMERCIAL / OFFICE BUILDING           |6               |AD        |6           |N               |02/20/2012    |220         |Burglary            |4910204051        |\n",
            "|20112510248    |THEFT BY SHOPLIFTING       |607                 |N              |09/08/2011  04:07 |09/08/2011   |407          |09/08/2011  04:12|09/08/2011 |412        |CONVENIENCE STORE                      |6               |AD        |5           |N               |10/05/2011    |23C         |Theft               |4910204081        |\n",
            "|20112551395    |ASSAULT BY CONTACT         |902                 |N              |09/12/2011  18:37 |09/12/2011   |1837         |09/12/2011  18:37|09/12/2011 |1837       |DRUG STORE / DOCTOR'S OFFICE / HOSPITAL|9               |BA        |5           |C               |09/12/2011    |NULL        |NULL                |4530002031        |\n",
            "|20115043519    |DEBIT CARD ABUSE           |1108                |N              |09/07/2011  12:00 |09/07/2011   |1200         |09/13/2011  18:11|09/13/2011 |1811       |RESIDENCE / HOME                       |9               |GE        |3           |N               |09/14/2011    |NULL        |NULL                |4530012001        |\n",
            "+---------------+---------------------------+--------------------+---------------+------------------+-------------+-------------+-----------------+-----------+-----------+---------------------------------------+----------------+----------+------------+----------------+--------------+------------+--------------------+------------------+\n",
            "only showing top 5 rows\n"
          ]
        }
      ],
      "source": [
        "# 1. Download Austin Crime Data (Real World Dataset)\n",
        "# Dataset: Austin Crime Reports (from data.austintexas.gov)\n",
        "# File size: ~500 MB\n",
        "!wget -O crime.csv \"https://data.austintexas.gov/api/views/fdj4-gpfu/rows.csv?accessType=DOWNLOAD\"\n",
        "\n",
        "print(\"‚úÖ Downloaded crime.csv\")\n",
        "\n",
        "# 2. Load CSV into Spark DataFrame\n",
        "# inferSchema=True: ‡πÉ‡∏´‡πâ Spark ‡πÄ‡∏î‡∏≤ Type ‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå (‡∏≠‡∏≤‡∏à‡∏ä‡πâ‡∏≤‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡∏ç‡πà‡∏°‡∏≤‡∏Å)\n",
        "# header=True: ‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡πÅ‡∏£‡∏Å‡∏Ñ‡∏∑‡∏≠‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå\n",
        "df_crime = spark.read.csv(\"crime.csv\", header=True, inferSchema=True)\n",
        "\n",
        "print(f\"‚úÖ Loaded Data: {df_crime.count():,} rows\")\n",
        "print(\"\\nüìä Original Schema:\")\n",
        "df_crime.printSchema()\n",
        "print(\"\\nüìã Sample Data:\")\n",
        "df_crime.show(5, truncate=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell_common_mistakes_load",
      "metadata": {
        "id": "cell_common_mistakes_load"
      },
      "source": [
        "### ‚ö†Ô∏è Common Pitfalls: Data Loading\n",
        "\n",
        "| ‚ùå ‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏Ñ‡∏ß‡∏£‡∏ó‡∏≥ | üí• ‡∏ú‡∏•‡πÄ‡∏™‡∏µ‡∏¢ | ‚úÖ ‡∏ó‡∏≥‡πÅ‡∏ö‡∏ö‡∏ô‡∏µ‡πâ‡πÅ‡∏ó‡∏ô |\n",
        "|---|---|---|\n",
        "| **inferSchema=True** ‡∏ö‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏£‡∏∞‡∏î‡∏±‡∏ö TB | Spark ‡∏ï‡πâ‡∏≠‡∏á‡∏≠‡πà‡∏≤‡∏ô‡∏ó‡∏±‡πâ‡∏á‡πÑ‡∏ü‡∏•‡πå 1 ‡∏£‡∏≠‡∏ö ‚Üí ‡∏ä‡πâ‡∏≤‡∏°‡∏≤‡∏Å | ‡∏Å‡∏≥‡∏´‡∏ô‡∏î `schema=struct` ‡πÄ‡∏≠‡∏á |\n",
        "| ‡∏•‡∏∑‡∏° `header=True` | ‡πÄ‡∏≠‡∏≤‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏°‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏ñ‡∏ß‡πÅ‡∏£‡∏Å | `option(\"header\", \"true\")` |\n",
        "| ‡πÑ‡∏ü‡∏•‡πå CSV ‡∏°‡∏µ Newline ‡πÉ‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° | ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏±‡∏á/‡∏≠‡πà‡∏≤‡∏ô‡∏ú‡∏¥‡∏î‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î | `option(\"multiLine\", \"true\")` |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell_parquet_ex_md",
      "metadata": {
        "id": "cell_parquet_ex_md"
      },
      "source": [
        "### ‚ö° Exercise 0: CSV vs Parquet\n",
        "\n",
        "**‡πÇ‡∏à‡∏ó‡∏¢‡πå:** ‡πÅ‡∏õ‡∏•‡∏á‡πÑ‡∏ü‡∏•‡πå CSV ‡πÄ‡∏õ‡πá‡∏ô Parquet ‡πÅ‡∏•‡πâ‡∏ß‡πÇ‡∏´‡∏•‡∏î‡∏Å‡∏•‡∏±‡∏ö‡∏°‡∏≤‡πÉ‡∏´‡∏°‡πà\n",
        "\n",
        "**‡∏ó‡∏≥‡πÑ‡∏°‡∏ï‡πâ‡∏≠‡∏á Parquet?**\n",
        "- ‡πÄ‡∏£‡πá‡∏ß‡∏Å‡∏ß‡πà‡∏≤ CSV ‡∏°‡∏≤‡∏Å (Column-oriented)\n",
        "- ‡πÄ‡∏Å‡πá‡∏ö Schema ‡πÉ‡∏ô‡∏ï‡∏±‡∏ß (‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á guess Type)\n",
        "- ‡∏Ç‡∏ô‡∏≤‡∏î‡πÄ‡∏•‡πá‡∏Å‡∏Å‡∏ß‡πà‡∏≤ (Compression)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "cell_parquet_ex_code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cell_parquet_ex_code",
        "outputId": "e9377ed3-9154-4a1b-99f2-8bd3af04a7f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Saved to crime_parquet\n",
            "‚úÖ Loaded Parquet: 2,619,292 rows\n"
          ]
        }
      ],
      "source": [
        "# TODO: ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÇ‡∏Ñ‡πâ‡∏î‡πÅ‡∏õ‡∏•‡∏á CSV -> Parquet\n",
        "# 1. Save as Parquet (‡∏ï‡∏±‡πâ‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå crime_parquet)\n",
        "\n",
        "df_crime.write.mode(\"overwrite\").parquet(\"crime_parquet\")\n",
        "\n",
        "print(\"‚úÖ Saved to crime_parquet\")\n",
        "\n",
        "# 2. Read from Parquet\n",
        "df_crime_parquet = spark.read.parquet(\"crime_parquet\")\n",
        "\n",
        "# 3. Compare Count\n",
        "print(f\"‚úÖ Loaded Parquet: {df_crime_parquet.count():,} rows\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell_step_2_trans",
      "metadata": {
        "id": "cell_step_2_trans"
      },
      "source": [
        "### Step 2: Transformation (Lazy)\n",
        "\n",
        "‡∏•‡∏≠‡∏á Filter ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏ö‡∏ö‡∏á‡πà‡∏≤‡∏¢‡πÜ (‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πàClean) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏π‡∏ß‡πà‡∏≤ Spark ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÅ‡∏ö‡∏ö Lazy Evaluation ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "cell_trans_demo",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cell_trans_demo",
        "outputId": "42f990db-03a3-433c-f0c5-85565288e4af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Transformation ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏™‡∏£‡πá‡∏à (Spark ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏à‡∏£‡∏¥‡∏á)\n"
          ]
        }
      ],
      "source": [
        "# ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ñ‡∏î‡∏µ 'THEFT' (‡∏•‡∏±‡∏Å‡∏ó‡∏£‡∏±‡∏û‡∏¢‡πå)\n",
        "# ‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏: ‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÉ‡∏ô Austin Dataset ‡∏Ñ‡∏∑‡∏≠ 'Highest Offense Description'\n",
        "df_theft = df_crime_parquet.filter(df_crime_parquet[\"Highest Offense Description\"] == \"THEFT\")\n",
        "\n",
        "print(\"‚úÖ Transformation ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏™‡∏£‡πá‡∏à (Spark ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏à‡∏£‡∏¥‡∏á)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell_step_3_action",
      "metadata": {
        "id": "cell_step_3_action"
      },
      "source": [
        "### Step 3: Action (Trigger)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "cell_action_demo",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cell_action_demo",
        "outputId": "2228b077-68fb-40f2-f343-0c1651b43f15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏î‡∏µ‡∏•‡∏±‡∏Å‡∏ó‡∏£‡∏±‡∏û‡∏¢‡πå (THEFT): 239,479\n",
            "‚è±Ô∏è ‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ: 1.01 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "t0 = time.time()\n",
        "count = df_theft.count()\n",
        "t1 = time.time()\n",
        "\n",
        "print(f\"üìä ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏î‡∏µ‡∏•‡∏±‡∏Å‡∏ó‡∏£‡∏±‡∏û‡∏¢‡πå (THEFT): {count:,}\")\n",
        "print(f\"‚è±Ô∏è ‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ: {t1-t0:.2f} ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell_common_mistakes_action",
      "metadata": {
        "id": "cell_common_mistakes_action"
      },
      "source": [
        "### ‚ö†Ô∏è Common Pitfalls: Actions\n",
        "\n",
        "| ‚ùå ‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏Ñ‡∏ß‡∏£‡∏£‡∏∞‡∏ß‡∏±‡∏á | ‚úÖ ‡∏ß‡∏¥‡∏ò‡∏µ‡πÅ‡∏Å‡πâ |\n",
        "|---|---|\n",
        "| **`collect()` ‡∏ö‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏ç‡πà** | Driver Memory ‡πÄ‡∏ï‡πá‡∏° (OOM) | ‡πÉ‡∏ä‡πâ `take(n)`, `show()`, ‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏•‡∏á Disk |\n",
        "| **`count()` ‡∏ö‡πà‡∏≠‡∏¢‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ** | ‡∏ó‡∏≥‡πÉ‡∏´‡πâ Spark ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏∏‡∏Å‡∏£‡∏≠‡∏ö (‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà Cache) | `count()` ‡πÄ‡∏ó‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell_step_4_explain",
      "metadata": {
        "id": "cell_step_4_explain"
      },
      "source": [
        "### Step 4: Execution Plan\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "cell_explain_demo",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cell_explain_demo",
        "outputId": "7630b438-a141-48fa-ed63-71b5775c992d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Physical Plan ==\n",
            "*(1) Filter (isnotnull(Highest Offense Description#263) AND (Highest Offense Description#263 = THEFT))\n",
            "+- *(1) ColumnarToRow\n",
            "   +- FileScan parquet [Incident Number#262L,Highest Offense Description#263,Highest Offense Code#264,Family Violence#265,Occurred Date Time#266,Occurred Date#267,Occurred Time#268,Report Date Time#269,Report Date#270,Report Time#271,Location Type#272,Council District#273,APD Sector#274,APD District#275,Clearance Status#276,Clearance Date#277,UCR Category#278,Category Description#279,Census Block Group#280L] Batched: true, DataFilters: [isnotnull(Highest Offense Description#263), (Highest Offense Description#263 = THEFT)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/content/crime_parquet], PartitionFilters: [], PushedFilters: [IsNotNull(`Highest Offense Description`), EqualTo(`Highest Offense Description`,THEFT)], ReadSchema: struct<Incident Number:bigint,Highest Offense Description:string,Highest Offense Code:int,Family ...\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_theft.explain(\"simple\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell_deep_dive_clean",
      "metadata": {
        "id": "cell_deep_dive_clean"
      },
      "source": [
        "# 8. üßπ Deep Dive: Data Cleaning & Preparation\n",
        "\n",
        "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏£‡∏¥‡∏á‡∏°‡∏±‡∏Å‡∏à‡∏∞‡πÑ‡∏°‡πà Clean! ‡πÄ‡∏£‡∏≤‡∏°‡∏≤‡∏•‡∏á‡∏°‡∏∑‡∏≠‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏±‡∏ô\n",
        "\n",
        "**‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏ó‡∏µ‡πà‡∏û‡∏ö:**\n",
        "1. ‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏¢‡∏≤‡∏ß‡πÅ‡∏•‡∏∞‡∏°‡∏µ‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á (‡πÄ‡∏ä‡πà‡∏ô `Highest Offense Description`) ‚Üí Rename ‡πÄ‡∏õ‡πá‡∏ô `crime_type`\n",
        "2. `Occurred Date Time` ‡πÄ‡∏õ‡πá‡∏ô String (e.g. `01/01/2022 12:00:00 PM`) ‚Üí Convert to Timestamp\n",
        "3. ‡πÅ‡∏¢‡∏Å `Year`, `Month` ‡∏≠‡∏≠‡∏Å‡∏°‡∏≤‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏á‡πà‡∏≤‡∏¢‡∏Ç‡∏∂‡πâ‡∏ô\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell_ex_clean_sql",
      "metadata": {
        "id": "cell_ex_clean_sql"
      },
      "source": [
        "### üõ†Ô∏è Exercise 1: Cleaning with Spark SQL\n",
        "\n",
        "**‡πÇ‡∏à‡∏ó‡∏¢‡πå:** ‡πÉ‡∏ä‡πâ Spark SQL ‡πÄ‡∏û‡∏∑‡πà‡∏≠:\n",
        "1. ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå\n",
        "2. ‡πÅ‡∏õ‡∏•‡∏á‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà\n",
        "3. ‡∏Å‡∏£‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏õ‡∏µ 2020 ‡∏Ç‡∏∂‡πâ‡∏ô‡πÑ‡∏õ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "cell_clean_sql_code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "cell_clean_sql_code",
        "outputId": "bb3bbfaf-0427-456c-d006-4ca9cf5ecebb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- case_id: long (nullable = true)\n",
            " |-- crime_type: string (nullable = true)\n",
            " |-- crime_date: timestamp (nullable = true)\n",
            " |-- census_block_group: long (nullable = true)\n",
            " |-- district: integer (nullable = true)\n",
            " |-- clearance_status: string (nullable = true)\n",
            "\n",
            "+-----------+--------------------+-------------------+------------------+--------+----------------+\n",
            "|    case_id|          crime_type|         crime_date|census_block_group|district|clearance_status|\n",
            "+-----------+--------------------+-------------------+------------------+--------+----------------+\n",
            "|20112440318|THEFT BY SHOPLIFTING|2011-09-01 04:55:00|        4530024371|       5|               N|\n",
            "|20112501319|BURGLARY NON RESI...|2011-09-02 03:02:00|        4910204051|       6|               N|\n",
            "|20112510248|THEFT BY SHOPLIFTING|2011-09-08 04:07:00|        4910204081|       6|               N|\n",
            "|20112551395|  ASSAULT BY CONTACT|2011-09-12 18:37:00|        4530002031|       9|               C|\n",
            "|20115043519|    DEBIT CARD ABUSE|2011-09-07 12:00:00|        4530012001|       9|               N|\n",
            "+-----------+--------------------+-------------------+------------------+--------+----------------+\n",
            "only showing top 5 rows\n"
          ]
        }
      ],
      "source": [
        "# 1. Register Temp View\n",
        "df_crime_parquet.createOrReplaceTempView(\"crime_raw\")\n",
        "\n",
        "# 2. Write SQL\n",
        "df_clean_sql = spark.sql(\"\"\"\n",
        "    SELECT\n",
        "        `Incident Number` as case_id,\n",
        "        `Highest Offense Description` as crime_type,\n",
        "        coalesce(try_to_timestamp(`Occurred Date Time`, 'MM/dd/yyyy  HH:mm'), try_to_timestamp(`Occurred Date Time`, 'MM/dd/yyyy hh:mm:ss a')) as crime_date,\n",
        "        `Census Block Group` as census_block_group,\n",
        "        `Council District` as district,\n",
        "        `Clearance Status` as clearance_status\n",
        "    FROM crime_raw\n",
        "    WHERE `Occurred Date Time` IS NOT NULL\n",
        "\"\"\")\n",
        "\n",
        "df_clean_sql.printSchema()\n",
        "df_clean_sql.show(5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0afca76",
      "metadata": {
        "id": "d0afca76"
      },
      "source": [
        "### üß© Exercise 1.2: SQL Challenge\n",
        "\n",
        "**‡πÇ‡∏à‡∏ó‡∏¢‡πå:** ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô SQL ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏≤ **Top 3 Districts** ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏î‡∏µ **BURGLARY** ‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î ‡πÉ‡∏ô‡∏õ‡∏µ **2023**\n",
        "\n",
        "‡πÄ‡∏ï‡∏¥‡∏°‡∏Ñ‡∏≥‡πÉ‡∏ô‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á (`____`) ‡πÉ‡∏´‡πâ‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_crime_parquet.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07GGTBbPFoTr",
        "outputId": "dd749e8a-57a6-4485-d31f-2c4551036228"
      },
      "id": "07GGTBbPFoTr",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+---------------------------+--------------------+---------------+------------------+-------------+-------------+-----------------+-----------+-----------+--------------------+----------------+----------+------------+----------------+--------------+------------+--------------------+------------------+\n",
            "|Incident Number|Highest Offense Description|Highest Offense Code|Family Violence|Occurred Date Time|Occurred Date|Occurred Time| Report Date Time|Report Date|Report Time|       Location Type|Council District|APD Sector|APD District|Clearance Status|Clearance Date|UCR Category|Category Description|Census Block Group|\n",
            "+---------------+---------------------------+--------------------+---------------+------------------+-------------+-------------+-----------------+-----------+-----------+--------------------+----------------+----------+------------+----------------+--------------+------------+--------------------+------------------+\n",
            "|    20112440318|       THEFT BY SHOPLIFTING|                 607|              N| 09/01/2011  04:55|   09/01/2011|          455|09/01/2011  04:55| 09/01/2011|        455|GROCERY / SUPERMA...|               5|        FR|           2|               N|    09/19/2011|         23C|               Theft|        4530024371|\n",
            "|    20112501319|       BURGLARY NON RESI...|                 502|              N| 09/02/2011  03:02|   09/02/2011|          302|09/07/2011  15:17| 09/07/2011|       1517|COMMERCIAL / OFFI...|               6|        AD|           6|               N|    02/20/2012|         220|            Burglary|        4910204051|\n",
            "|    20112510248|       THEFT BY SHOPLIFTING|                 607|              N| 09/08/2011  04:07|   09/08/2011|          407|09/08/2011  04:12| 09/08/2011|        412|   CONVENIENCE STORE|               6|        AD|           5|               N|    10/05/2011|         23C|               Theft|        4910204081|\n",
            "|    20112551395|         ASSAULT BY CONTACT|                 902|              N| 09/12/2011  18:37|   09/12/2011|         1837|09/12/2011  18:37| 09/12/2011|       1837|DRUG STORE / DOCT...|               9|        BA|           5|               C|    09/12/2011|        NULL|                NULL|        4530002031|\n",
            "|    20115043519|           DEBIT CARD ABUSE|                1108|              N| 09/07/2011  12:00|   09/07/2011|         1200|09/13/2011  18:11| 09/13/2011|       1811|    RESIDENCE / HOME|               9|        GE|           3|               N|    09/14/2011|        NULL|                NULL|        4530012001|\n",
            "+---------------+---------------------------+--------------------+---------------+------------------+-------------+-------------+-----------------+-----------+-----------+--------------------+----------------+----------+------------+----------------+--------------+------------+--------------------+------------------+\n",
            "only showing top 5 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "44f566ab",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 671
        },
        "collapsed": true,
        "id": "44f566ab",
        "outputId": "c6cedeb8-3f15-4478-fcb0-8bb730b83331"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "{\"ts\": \"2026-02-14 07:56:51.339\", \"level\": \"ERROR\", \"logger\": \"SQLQueryContextLogger\", \"msg\": \"[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `crime_type` cannot be resolved. Did you mean one of the following? [`APD Sector`, `Location Type`, `Report Date`, `Report Time`, `Occurred Date`]. SQLSTATE: 42703\", \"context\": {\"errorClass\": \"UNRESOLVED_COLUMN.WITH_SUGGESTION\"}, \"exception\": {\"class\": \"Py4JJavaError\", \"msg\": \"An error occurred while calling o31.sql.\\n: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `crime_type` cannot be resolved. Did you mean one of the following? [`APD Sector`, `Location Type`, `Report Date`, `Report Time`, `Occurred Date`]. SQLSTATE: 42703; line 6 pos 10;\\n'GlobalLimit 3\\n+- 'LocalLimit 3\\n   +- 'Sort ['crime_type DESC NULLS LAST], true\\n      +- 'Aggregate ['district], ['Location Type AS district#489, count(1) AS crime_count#490L]\\n         +- 'Filter (('crime_type = BURGLARY) AND ('year = 2023))\\n            +- SubqueryAlias crime_raw\\n               +- View (`crime_raw`, [Incident Number#262L, Highest Offense Description#263, Highest Offense Code#264, Family Violence#265, Occurred Date Time#266, Occurred Date#267, Occurred Time#268, Report Date Time#269, Report Date#270, Report Time#271, Location Type#272, Council District#273, APD Sector#274, APD District#275, Clearance Status#276, Clearance Date#277, UCR Category#278, Category Description#279, Census Block Group#280L])\\n                  +- Relation [Incident Number#262L,Highest Offense Description#263,Highest Offense Code#264,Family Violence#265,Occurred Date Time#266,Occurred Date#267,Occurred Time#268,Report Date Time#269,Report Date#270,Report Time#271,Location Type#272,Council District#273,APD Sector#274,APD District#275,Clearance Status#276,Clearance Date#277,UCR Category#278,Category Description#279,Census Block Group#280L] parquet\\n\\n\\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:401)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:169)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7(CheckAnalysis.scala:404)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7$adapted(CheckAnalysis.scala:402)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:251)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:251)\\n\\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:251)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:251)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:251)\\n\\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:251)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:402)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:402)\\n\\tat scala.collection.immutable.List.foreach(List.scala:334)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:402)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:284)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:251)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:251)\\n\\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:251)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:251)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:251)\\n\\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:251)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:251)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:251)\\n\\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:251)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:251)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:251)\\n\\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:251)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:284)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:255)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:299)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:244)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:231)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:299)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:192)\\n\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\\n\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:192)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\\n\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\\n\\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\\n\\tat scala.util.Try$.apply(Try.scala:217)\\n\\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\\n\\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)\\n\\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\\n\\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)\\n\\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)\\n\\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$5(Dataset.scala:139)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:136)\\n\\tat org.apache.spark.sql.classic.SparkSession.$anonfun$sql$1(SparkSession.scala:462)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\tat org.apache.spark.sql.classic.SparkSession.sql(SparkSession.scala:449)\\n\\tat org.apache.spark.sql.classic.SparkSession.sql(SparkSession.scala:467)\\n\\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n\\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\\n\\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\\n\\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\\n\\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\\n\\tat py4j.Gateway.invoke(Gateway.java:282)\\n\\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\\n\\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\\n\\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\\n\\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\\n\\tat java.base/java.lang.Thread.run(Thread.java:840)\\n\\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\\n\\t\\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:401)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:169)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7(CheckAnalysis.scala:404)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7$adapted(CheckAnalysis.scala:402)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:251)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:251)\\n\\t\\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:251)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:251)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:251)\\n\\t\\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:251)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:402)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:402)\\n\\t\\tat scala.collection.immutable.List.foreach(List.scala:334)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:402)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:284)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:251)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:251)\\n\\t\\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:251)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:251)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:251)\\n\\t\\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:251)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:251)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:251)\\n\\t\\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:251)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:251)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:251)\\n\\t\\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:251)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:284)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:255)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:299)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:244)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:231)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:299)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:192)\\n\\t\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\\n\\t\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:192)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)\\n\\t\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\\n\\t\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\\n\\t\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\\n\\t\\tat scala.util.Try$.apply(Try.scala:217)\\n\\t\\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\\n\\t\\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\\n\\t\\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\\n\\t\\t... 22 more\\n\", \"stacktrace\": [{\"class\": null, \"method\": \"deco\", \"file\": \"/usr/local/lib/python3.12/dist-packages/pyspark/errors/exceptions/captured.py\", \"line\": \"282\"}, {\"class\": null, \"method\": \"get_return_value\", \"file\": \"/usr/local/lib/python3.12/dist-packages/py4j/protocol.py\", \"line\": \"327\"}]}}\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `crime_type` cannot be resolved. Did you mean one of the following? [`APD Sector`, `Location Type`, `Report Date`, `Report Time`, `Occurred Date`]. SQLSTATE: 42703; line 6 pos 10;\n'GlobalLimit 3\n+- 'LocalLimit 3\n   +- 'Sort ['crime_type DESC NULLS LAST], true\n      +- 'Aggregate ['district], ['Location Type AS district#489, count(1) AS crime_count#490L]\n         +- 'Filter (('crime_type = BURGLARY) AND ('year = 2023))\n            +- SubqueryAlias crime_raw\n               +- View (`crime_raw`, [Incident Number#262L, Highest Offense Description#263, Highest Offense Code#264, Family Violence#265, Occurred Date Time#266, Occurred Date#267, Occurred Time#268, Report Date Time#269, Report Date#270, Report Time#271, Location Type#272, Council District#273, APD Sector#274, APD District#275, Clearance Status#276, Clearance Date#277, UCR Category#278, Category Description#279, Census Block Group#280L])\n                  +- Relation [Incident Number#262L,Highest Offense Description#263,Highest Offense Code#264,Family Violence#265,Occurred Date Time#266,Occurred Date#267,Occurred Time#268,Report Date Time#269,Report Date#270,Report Time#271,Location Type#272,Council District#273,APD Sector#274,APD District#275,Clearance Status#276,Clearance Date#277,UCR Category#278,Category Description#279,Census Block Group#280L] parquet\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1641052833.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msql_challenge\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1808\u001b[0m                     \u001b[0mmessageParameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"arg_name\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"args\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"arg_type\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1809\u001b[0m                 )\n\u001b[0;32m-> 1810\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlitArgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1811\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1812\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1362\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1363\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    286\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `crime_type` cannot be resolved. Did you mean one of the following? [`APD Sector`, `Location Type`, `Report Date`, `Report Time`, `Occurred Date`]. SQLSTATE: 42703; line 6 pos 10;\n'GlobalLimit 3\n+- 'LocalLimit 3\n   +- 'Sort ['crime_type DESC NULLS LAST], true\n      +- 'Aggregate ['district], ['Location Type AS district#489, count(1) AS crime_count#490L]\n         +- 'Filter (('crime_type = BURGLARY) AND ('year = 2023))\n            +- SubqueryAlias crime_raw\n               +- View (`crime_raw`, [Incident Number#262L, Highest Offense Description#263, Highest Offense Code#264, Family Violence#265, Occurred Date Time#266, Occurred Date#267, Occurred Time#268, Report Date Time#269, Report Date#270, Report Time#271, Location Type#272, Council District#273, APD Sector#274, APD District#275, Clearance Status#276, Clearance Date#277, UCR Category#278, Category Description#279, Census Block Group#280L])\n                  +- Relation [Incident Number#262L,Highest Offense Description#263,Highest Offense Code#264,Family Violence#265,Occurred Date Time#266,Occurred Date#267,Occurred Time#268,Report Date Time#269,Report Date#270,Report Time#271,Location Type#272,Council District#273,APD Sector#274,APD District#275,Clearance Status#276,Clearance Date#277,UCR Category#278,Category Description#279,Census Block Group#280L] parquet\n"
          ]
        }
      ],
      "source": [
        "# TODO: ‡πÄ‡∏ï‡∏¥‡∏°‡∏Ñ‡∏≥‡πÉ‡∏ô‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á\n",
        "sql_challenge = \"\"\"\n",
        "    SELECT\n",
        "        `Location Type` as district,\n",
        "        COUNT(*) as crime_count\n",
        "    FROM crime_raw\n",
        "    WHERE crime_type = 'BURGLARY'\n",
        "      AND year = 2023\n",
        "    GROUP BY district\n",
        "    ORDER BY crime_type DESC\n",
        "    LIMIT 3\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "spark.sql(sql_challenge).show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: ‡πÄ‡∏ï‡∏¥‡∏°‡∏Ñ‡∏≥‡πÉ‡∏ô‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á\n",
        "sql_challenge = \"\"\"\n",
        "    SELECT\n",
        "        `Location Type` as district,\n",
        "        COUNT(*) as crime_count\n",
        "    FROM crime_raw\n",
        "    WHERE `Highest Offense Description` = 'BURGLARY'\n",
        "      AND YEAR(to_timestamp(`Occurred Date Time`, 'MM/dd/yyyy  HH:mm')) = 2023\n",
        "    GROUP BY district\n",
        "    ORDER BY crime_count DESC\n",
        "    LIMIT 3\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "spark.sql(sql_challenge).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WmFt2ErdKRpD",
        "outputId": "965ac0b8-cb33-4a89-8e5c-87dafb37c9be"
      },
      "id": "WmFt2ErdKRpD",
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-----------+\n",
            "|district|crime_count|\n",
            "+--------+-----------+\n",
            "+--------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell_ex_clean_py",
      "metadata": {
        "id": "cell_ex_clean_py"
      },
      "source": [
        "### üõ†Ô∏è Exercise 2: Cleaning with PySpark Functions\n",
        "\n",
        "**‡πÇ‡∏à‡∏ó‡∏¢‡πå:** ‡∏ó‡∏≥‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Ç‡πâ‡∏≠‡∏ö‡∏ô ‡πÅ‡∏ï‡πà‡πÉ‡∏ä‡πâ `pyspark.sql.functions` ‡πÅ‡∏ó‡∏ô SQL Strings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "cell_clean_py_code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cell_clean_py_code",
        "outputId": "2e490a51-ff37-4b7f-e219-e6ffe65bd900"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Cleaned Data Structure:\n",
            "root\n",
            " |-- case_id: long (nullable = true)\n",
            " |-- crime_type: string (nullable = true)\n",
            " |-- crime_date: timestamp (nullable = true)\n",
            " |-- census_block_group: string (nullable = true)\n",
            " |-- district: integer (nullable = true)\n",
            " |-- clearance_status: string (nullable = true)\n",
            " |-- year: integer (nullable = true)\n",
            " |-- month: integer (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "df_clean = df_crime_parquet.select(\n",
        "    F.col(\"Incident Number\").alias(\"case_id\"),\n",
        "    F.col(\"Highest Offense Description\").alias(\"crime_type\"),\n",
        "    F.coalesce(F.to_timestamp(\"Occurred Date Time\", \"MM/dd/yyyy  HH:mm\"), F.to_timestamp(\"Occurred Date Time\", \"MM/dd/yyyy hh:mm:ss a\")).alias(\"crime_date\"),\n",
        "    F.col(\"Census Block Group\").cast(\"string\").alias(\"census_block_group\"),\n",
        "    F.col(\"Council District\").alias(\"district\"),\n",
        "    F.col(\"Clearance Status\").alias(\"clearance_status\"),\n",
        ") \\\n",
        ".withColumn(\"year\", F.year(\"crime_date\")) \\\n",
        ".withColumn(\"month\", F.month(\"crime_date\")) \\\n",
        ".filter(F.col(\"crime_date\").isNotNull())\n",
        "\n",
        "print(\"‚úÖ Cleaned Data Structure:\")\n",
        "df_clean.printSchema()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "606aa2d9",
      "metadata": {
        "id": "606aa2d9"
      },
      "source": [
        "### üß© Exercise 2.2: PySpark Transformations\n",
        "\n",
        "**‡πÇ‡∏à‡∏ó‡∏¢‡πå:** ‡πÉ‡∏ä‡πâ PySpark ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏£‡∏≠‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ñ‡∏î‡∏µ‡∏ó‡∏µ‡πà **Clearance Status** ‡πÑ‡∏°‡πà‡πÄ‡∏õ‡πá‡∏ô NULL ‡πÅ‡∏•‡∏∞‡πÅ‡∏™‡∏î‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå `case_id`, `crime_type`, `district`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "06bad55d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06bad55d",
        "outputId": "c123b204-fdcc-48f5-f348-116c8dc24f77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+--------------------+--------+\n",
            "|    case_id|          crime_type|district|\n",
            "+-----------+--------------------+--------+\n",
            "|20112440318|THEFT BY SHOPLIFTING|       5|\n",
            "|20112501319|BURGLARY NON RESI...|       6|\n",
            "|20112510248|THEFT BY SHOPLIFTING|       6|\n",
            "|20112551395|  ASSAULT BY CONTACT|       9|\n",
            "|20115043519|    DEBIT CARD ABUSE|       9|\n",
            "+-----------+--------------------+--------+\n",
            "only showing top 5 rows\n"
          ]
        }
      ],
      "source": [
        "# TODO: ‡πÄ‡∏ï‡∏¥‡∏°‡∏Ñ‡∏≥‡πÉ‡∏ô‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á\n",
        "df_challenge = df_clean.select(\n",
        "    F.col(\"case_id\"),\n",
        "    F.col(\"crime_type\"),\n",
        "    F.col(\"district\")\n",
        ").filter(\n",
        "    F.col(\"clearance_status\").isNotNull()\n",
        ")\n",
        "\n",
        "df_challenge.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_clean.show(5)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "oZvdSCr8K-L8",
        "outputId": "65c2716c-3e4f-4725-d415-98f9fe3fe080",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "oZvdSCr8K-L8",
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+--------------------+-------------------+------------------+--------+----------------+----+-----+\n",
            "|    case_id|          crime_type|         crime_date|census_block_group|district|clearance_status|year|month|\n",
            "+-----------+--------------------+-------------------+------------------+--------+----------------+----+-----+\n",
            "|20112440318|THEFT BY SHOPLIFTING|2011-09-01 04:55:00|        4530024371|       5|               N|2011|    9|\n",
            "|20112501319|BURGLARY NON RESI...|2011-09-02 03:02:00|        4910204051|       6|               N|2011|    9|\n",
            "|20112510248|THEFT BY SHOPLIFTING|2011-09-08 04:07:00|        4910204081|       6|               N|2011|    9|\n",
            "|20112551395|  ASSAULT BY CONTACT|2011-09-12 18:37:00|        4530002031|       9|               C|2011|    9|\n",
            "|20115043519|    DEBIT CARD ABUSE|2011-09-07 12:00:00|        4530012001|       9|               N|2011|    9|\n",
            "+-----------+--------------------+-------------------+------------------+--------+----------------+----+-----+\n",
            "only showing top 5 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell_common_mistakes_trans",
      "metadata": {
        "id": "cell_common_mistakes_trans"
      },
      "source": [
        "### ‚ö†Ô∏è Common Pitfalls: Transformation\n",
        "\n",
        "| ‚ùå ‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏Ñ‡∏ß‡∏£‡∏£‡∏∞‡∏ß‡∏±‡∏á | ‚úÖ ‡∏ß‡∏¥‡∏ò‡∏µ‡πÅ‡∏Å‡πâ |\n",
        "|---|---|\n",
        "| **Chaining `withColumn`** ‡∏°‡∏≤‡∏Å‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ | Spark ‡∏à‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á plan ‡∏ó‡∏µ‡πà‡∏ã‡πâ‡∏≠‡∏ô‡∏Å‡∏±‡∏ô‡∏•‡∏∂‡∏Å‡∏°‡∏≤‡∏Å ‚Üí ‡∏ä‡πâ‡∏≤ | ‡πÉ‡∏ä‡πâ `select()` ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏´‡∏•‡∏≤‡∏¢‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß |\n",
        "| ‡πÉ‡∏ä‡πâ **Python Function (UDF)** | ‡∏ä‡πâ‡∏≤‡∏°‡∏≤‡∏Å ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏ï‡πâ‡∏≠‡∏á‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡πâ‡∏≤‡∏°‡πÑ‡∏õ Python | ‡πÉ‡∏ä‡πâ **Spark Built-in Functions** (`F.xx`) ‡πÄ‡∏™‡∏°‡∏≠ |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell_analysis_ex",
      "metadata": {
        "id": "cell_analysis_ex"
      },
      "source": [
        "# 9. üìä Analysis Exercises\n",
        "\n",
        "‡πÉ‡∏ä‡πâ `df_clean` ‡∏ó‡∏µ‡πà‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡πÑ‡∏ß‡πâ ‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "cell_ex_top_crimes",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cell_ex_top_crimes",
        "outputId": "5f8b4f19-f9b3-4226-822f-90542fe36a59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------------------------+------+\n",
            "|crime_type                        |count |\n",
            "+----------------------------------+------+\n",
            "|BURGLARY OF VEHICLE               |266983|\n",
            "|THEFT                             |239478|\n",
            "|FAMILY DISTURBANCE                |222524|\n",
            "|CRIMINAL MISCHIEF                 |150427|\n",
            "|ASSAULT WITH INJURY-FAM/DATING VIO|93653 |\n",
            "+----------------------------------+------+\n",
            "only showing top 5 rows\n"
          ]
        }
      ],
      "source": [
        "# ‡πÇ‡∏à‡∏ó‡∏¢‡πå 1: 5 ‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö‡∏≠‡∏≤‡∏ä‡∏ç‡∏≤‡∏Å‡∏£‡∏£‡∏°‡∏ó‡∏µ‡πà‡∏û‡∏ö‡∏ö‡πà‡∏≠‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î\n",
        "df_clean.groupBy(\"crime_type\").count().orderBy(F.col(\"count\").desc()).show(5, truncate=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "cell_ex_year_trend",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cell_ex_year_trend",
        "outputId": "6602adc4-b99e-4e52-8c12-20408691fa03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+------+\n",
            "|year| count|\n",
            "+----+------+\n",
            "|2003|120970|\n",
            "|2004|119747|\n",
            "|2005|127855|\n",
            "|2006|129144|\n",
            "|2007|136747|\n",
            "|2008|143488|\n",
            "|2009|141208|\n",
            "|2010|136518|\n",
            "|2011|127951|\n",
            "|2012|127426|\n",
            "|2013|122982|\n",
            "|2014|114739|\n",
            "|2015|110704|\n",
            "|2016|107057|\n",
            "|2017|103805|\n",
            "|2018| 97932|\n",
            "|2019|103075|\n",
            "|2020| 98873|\n",
            "|2021| 91622|\n",
            "|2022| 88106|\n",
            "+----+------+\n",
            "only showing top 20 rows\n"
          ]
        }
      ],
      "source": [
        "# ‡πÇ‡∏à‡∏ó‡∏¢‡πå 2: ‡πÅ‡∏ô‡∏ß‡πÇ‡∏ô‡πâ‡∏°‡∏≠‡∏≤‡∏ä‡∏ç‡∏≤‡∏Å‡∏£‡∏£‡∏°‡∏£‡∏≤‡∏¢‡∏õ‡∏µ (Count by Year)\n",
        "df_clean.groupBy(\"year\").count().orderBy(\"year\").show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "cell_ex_clearance",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cell_ex_clearance",
        "outputId": "ff832fa0-b5a1-4572-e538-3d8a993e62d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+-------+\n",
            "|clearance_status|count  |\n",
            "+----------------+-------+\n",
            "|N               |1310951|\n",
            "|NULL            |632684 |\n",
            "|C               |592370 |\n",
            "|O               |83222  |\n",
            "|9               |1      |\n",
            "+----------------+-------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ‡πÇ‡∏à‡∏ó‡∏¢‡πå 3: ‡∏™‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏¥‡∏î‡∏Ñ‡∏î‡∏µ (Clearance Status)\n",
        "df_clean.groupBy(\"clearance_status\").count().orderBy(F.col(\"count\").desc()).show(truncate=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f285d68d",
      "metadata": {
        "id": "f285d68d"
      },
      "source": [
        "### üß© Analysis Challenge (Fill-in-the-blank)\n",
        "\n",
        "‡πÄ‡∏ï‡∏¥‡∏°‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡πÉ‡∏´‡πâ‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "484992c3",
      "metadata": {
        "id": "484992c3"
      },
      "outputs": [],
      "source": [
        "# 4. ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏î‡∏π‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ã‡πâ‡∏≥‡∏Å‡∏±‡∏ô‡∏Ç‡∏≠‡∏á crime_type\n",
        "df_clean.select(\"crime_type\").______().count()\n",
        "\n",
        "# 5. ‡∏Å‡∏≤‡∏£ Join ‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÄ‡∏•‡πá‡∏Å (Lookup) ‡∏Å‡∏±‡∏ö‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÉ‡∏´‡∏ç‡πà ‡∏Ñ‡∏ß‡∏£‡πÉ‡∏ä‡πâ‡∏≠‡∏∞‡πÑ‡∏£‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î‡∏Å‡∏≤‡∏£ Shuffle?\n",
        "df_large.join(F.______(df_small), \"id\")\n",
        "\n",
        "# 6. ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏•‡∏¥‡∏Å‡πÉ‡∏ä‡πâ DataFrame ‡∏ó‡∏µ‡πà Cache ‡πÑ‡∏ß‡πâ ‡∏Ñ‡∏ß‡∏£‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏≠‡∏∞‡πÑ‡∏£‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏∑‡∏ô RAM?\n",
        "df.______() # hint: ‡∏ï‡∏£‡∏á‡∏Ç‡πâ‡∏≤‡∏°‡∏Å‡∏±‡∏ö persist"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell_cache_perf",
      "metadata": {
        "id": "cell_cache_perf"
      },
      "source": [
        "# 10. üöÄ Cache Performance Test\n",
        "\n",
        "‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ Cache ‡∏Å‡∏±‡∏ö DataFrame ‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô‡∏Å‡∏≤‡∏£ Clean ‡∏°‡∏≤‡πÅ‡∏•‡πâ‡∏ß\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell_cache_code",
      "metadata": {
        "id": "cell_cache_code"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "# Force Transformation\n",
        "df_heavy = df_clean.filter(F.col(\"year\") >= 2020).groupBy(\"district\").count()\n",
        "\n",
        "# 1. No Cache\n",
        "t0 = time.time()\n",
        "df_heavy.show(5) # Action 1\n",
        "df_heavy.count() # Action 2\n",
        "t1 = time.time()\n",
        "print(f\"üî¥ No Cache: {t1-t0:.2f} sec\")\n",
        "\n",
        "# 2. With Cache\n",
        "df_clean.cache()\n",
        "t2 = time.time()\n",
        "df_heavy.show(5) # Action 1 (First time builds cache)\n",
        "df_heavy.count() # Action 2 (Hits cache)\n",
        "t3 = time.time()\n",
        "print(f\"üü¢ Cache: {t3-t2:.2f} sec\")\n",
        "\n",
        "df_clean.unpersist()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell_common_mistakes_perf",
      "metadata": {
        "id": "cell_common_mistakes_perf"
      },
      "source": [
        "### ‚ö†Ô∏è Common Pitfalls: Performance\n",
        "\n",
        "| ‚ùå ‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏Ñ‡∏ß‡∏£‡∏£‡∏∞‡∏ß‡∏±‡∏á | ‚úÖ ‡∏ß‡∏¥‡∏ò‡∏µ‡πÅ‡∏Å‡πâ |\n",
        "|---|---|\n",
        "| **‡∏•‡∏∑‡∏° `unpersist()`** | RAM ‡πÄ‡∏ï‡πá‡∏°‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô | `unpersist()` ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏•‡∏¥‡∏Å‡πÉ‡∏ä‡πâ DataFrame ‡∏ô‡∏±‡πâ‡∏ô‡πÅ‡∏•‡πâ‡∏ß |\n",
        "| **Cache ‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÉ‡∏´‡∏ç‡πà‡πÄ‡∏Å‡∏¥‡∏ô RAM** | Disk Spill (‡∏ä‡πâ‡∏≤‡∏Å‡∏ß‡πà‡∏≤‡πÄ‡∏î‡∏¥‡∏°) | ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Cache ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏ã‡πâ‡∏≥‡∏ö‡πà‡∏≠‡∏¢‡πÜ ‡πÅ‡∏•‡∏∞‡πÑ‡∏°‡πà‡πÉ‡∏´‡∏ç‡πà‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ |\n",
        "| **Shuffle ‡∏ö‡∏ô Key ‡∏ó‡∏µ‡πà‡πÄ‡∏ö‡∏µ‡πâ‡∏¢‡∏ß (Skew)** | ‡∏ö‡∏≤‡∏á‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏´‡∏ô‡∏±‡∏Å‡∏ï‡∏≤‡∏¢‡∏á | ‡πÉ‡∏ä‡πâ Salt Key ‡∏´‡∏£‡∏∑‡∏≠ Broadcast Join (‡∏ñ‡πâ‡∏≤‡∏ó‡∏≥‡πÑ‡∏î‡πâ) |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell_bonus_map",
      "metadata": {
        "id": "cell_bonus_map"
      },
      "source": [
        "# 11. üó∫Ô∏è Bonus: Visualizing Crime on Map\n",
        "\n",
        "**Goal:** Plot ‡∏à‡∏∏‡∏î‡πÄ‡∏Å‡∏¥‡∏î‡πÄ‡∏´‡∏ï‡∏∏‡∏ö‡∏ô‡πÅ‡∏ú‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏£‡∏¥‡∏á‡∏î‡πâ‡∏ß‡∏¢ `folium`\n",
        "\n",
        "**‡∏Ñ‡∏≥‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô:** ‡πÄ‡∏£‡∏≤‡∏à‡∏∞ Plot ‡πÅ‡∏Ñ‡πà **500 ‡∏à‡∏∏‡∏î** ‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô Browser ‡∏Ñ‡πâ‡∏≤‡∏á\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell_map_code",
      "metadata": {
        "id": "cell_map_code"
      },
      "outputs": [],
      "source": [
        "# 1. Download Austin Census Block Groups GeoJSON\n",
        "import requests\n",
        "import json\n",
        "import folium\n",
        "\n",
        "geojson_url = \"https://data.austintexas.gov/api/views/dwa9-qvcr/rows.geojson?accessType=DOWNLOAD\"\n",
        "\n",
        "print(\"‚è≥ Downloading GeoJSON...\")\n",
        "try:\n",
        "    resp = requests.get(geojson_url)\n",
        "    austin_geo = resp.json()\n",
        "    print(\"‚úÖ Downloaded GeoJSON\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Failed to download GeoJSON: {e}\")\n",
        "    austin_geo = None\n",
        "\n",
        "if austin_geo:\n",
        "    # 2. Prepare Data\n",
        "    df_heatmap = df_clean.filter(F.col(\"census_block_group\").isNotNull()) \\\n",
        "                         .groupBy(\"census_block_group\").count() \\\n",
        "                         .toPandas()\n",
        "\n",
        "    # DEBUG: Check values\n",
        "    print(\"üîç Data Sample:\", df_heatmap['census_block_group'].head(5).tolist())\n",
        "    print(\"üîç GeoJSON Sample:\", [f['properties']['geoid'] for f in austin_geo['features'][:5]])\n",
        "\n",
        "    df_heatmap['census_block_group'] = df_heatmap['census_block_group'].astype(str)\n",
        "\n",
        "    # 3. Create Map\n",
        "    austin_map = folium.Map(location=[30.2672, -97.7431], zoom_start=11)\n",
        "\n",
        "    folium.Choropleth(\n",
        "        geo_data=austin_geo,\n",
        "        name=\"Crime Density\",\n",
        "        data=df_heatmap,\n",
        "        columns=[\"census_block_group\", \"count\"],\n",
        "        key_on=\"feature.properties.geoid\",\n",
        "        fill_color=\"YlOrRd\",\n",
        "        fill_opacity=0.7,\n",
        "        line_opacity=0.2,\n",
        "        nan_fill_color=\"white\",\n",
        "        legend_name=\"Crime Count per Block Group\"\n",
        "    ).add_to(austin_map)\n",
        "\n",
        "    folium.LayerControl().add_to(austin_map)\n",
        "    austin_map\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e526d2e",
      "metadata": {
        "id": "3e526d2e"
      },
      "outputs": [],
      "source": [
        "folium.LayerControl().add_to(austin_map)\n",
        "austin_map"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell_45",
      "metadata": {
        "id": "cell_45"
      },
      "source": [
        "# 12. üìå ‡∏™‡∏£‡∏∏‡∏õ (Summary)\n",
        "\n",
        "## ‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ\n",
        "\n",
        "| ‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠ | ‡∏™‡∏£‡∏∏‡∏õ |\n",
        "|--------|------|\n",
        "| **‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡πÑ‡∏°‡πà‡∏û‡∏≠** | ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• > RAM/CPU ‚Üí ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ Distributed Computing |\n",
        "| **Spark ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£** | Distributed Compute Engine (‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà DB, ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà Storage) |\n",
        "| **Architecture** | Driver (‡∏™‡∏°‡∏≠‡∏á) + Executors (‡πÅ‡∏£‡∏á‡∏á‡∏≤‡∏ô) + Cluster Manager (‡∏ú‡∏π‡πâ‡∏à‡∏±‡∏î‡∏™‡∏£‡∏£) |\n",
        "| **Lazy Evaluation** | Transformation ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏ú‡∏ô, Action ‡∏™‡∏±‡πà‡∏á‡∏ó‡∏≥ ‚Üí ‡∏ä‡πà‡∏ß‡∏¢ optimize |\n",
        "| **DAG** | ‡∏Å‡∏£‡∏≤‡∏ü‡πÅ‡∏™‡∏î‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô ‚Üí Catalyst Optimizer ‡∏õ‡∏£‡∏±‡∏ö‡πÉ‡∏´‡πâ‡πÄ‡∏£‡πá‡∏ß |\n",
        "| **Shuffle** | ‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡πâ‡∏≤‡∏°‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á ‚Üí ‡∏ä‡πâ‡∏≤ ‡πÅ‡∏û‡∏á ‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏∞‡∏ß‡∏±‡∏á |\n",
        "| **Cache** | ‡πÄ‡∏Å‡πá‡∏ö DataFrame ‡πÉ‡∏ô RAM ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÉ‡∏ä‡πâ‡∏ã‡πâ‡∏≥ ‚Üí ‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î‡πÄ‡∏ß‡∏•‡∏≤ |\n",
        "| **explain()** | ‡∏î‡∏π‡πÅ‡∏ú‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô ‚Üí ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏ß‡πà‡∏≤ Spark ‡∏à‡∏∞‡∏ó‡∏≥‡∏≠‡∏∞‡πÑ‡∏£ |\n",
        "\n",
        "## üîÆ ‡∏™‡∏±‡∏õ‡∏î‡∏≤‡∏´‡πå‡∏´‡∏ô‡πâ‡∏≤: Data Pipeline\n",
        "\n",
        "> ‡∏™‡∏±‡∏õ‡∏î‡∏≤‡∏´‡πå‡∏ó‡∏µ‡πà 5: ‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (Data Pipeline)  \n",
        "> ingest ‚Üí clean ‚Üí transform ‚Üí store ‚Üí analyze\n",
        "\n",
        "---\n",
        "\n",
        "## üìö ‡πÅ‡∏´‡∏•‡πà‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°\n",
        "\n",
        "- [Apache Spark Documentation](https://spark.apache.org/docs/latest/)\n",
        "- [Spark: The Definitive Guide](https://www.oreilly.com/library/view/spark-the-definitive/9781491912201/)\n",
        "- [PySpark API Reference](https://spark.apache.org/docs/latest/api/python/)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell_46",
      "metadata": {
        "id": "cell_46"
      },
      "outputs": [],
      "source": [
        "# ‡∏õ‡∏¥‡∏î Spark Session\n",
        "# spark.stop() # Comment ‡∏≠‡∏≠‡∏Å‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ó‡∏≥ Lab Test ‡∏ï‡πà‡∏≠\n",
        "print(\"‚úÖ ‡∏à‡∏ö‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏´‡∏•‡∏±‡∏Å! ‡πÑ‡∏õ‡∏•‡∏∏‡∏¢ Lab Test ‡∏Å‡∏±‡∏ô‡∏ï‡πà‡∏≠ üöÄ\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "abd71b47",
      "metadata": {
        "id": "abd71b47"
      },
      "source": [
        "# 13. üß™ Lab Test: Olist E-Commerce Analytics (Real World Data)\n",
        "\n",
        "**Dataset:** Brazilian E-Commerce Public Dataset by Olist  \n",
        "**Source:** [Kaggle / GitHub](https://github.com/ayushic2899/Brazilian-E-Commerce-Public-Dataset-by-Olist)  \n",
        "**Goal:** ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏¢‡∏≠‡∏î‡∏Ç‡∏≤‡∏¢‡πÅ‡∏•‡∏∞‡∏û‡∏§‡∏ï‡∏¥‡∏Å‡∏£‡∏£‡∏°‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏£‡∏¥‡∏á 3 ‡∏ï‡∏≤‡∏£‡∏≤‡∏á (Orders, Items, Products)\n",
        "\n",
        "### üì• Step 0: Download Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af772ccc",
      "metadata": {
        "id": "af772ccc"
      },
      "outputs": [],
      "source": [
        "# Download Dataset from GitHub\n",
        "!wget -q -O olist_orders.csv https://raw.githubusercontent.com/ayushic2899/Brazilian-E-Commerce-Public-Dataset-by-Olist/master/olist_orders_dataset.csv\n",
        "!wget -q -O olist_items.csv https://raw.githubusercontent.com/ayushic2899/Brazilian-E-Commerce-Public-Dataset-by-Olist/master/olist_order_items_dataset.csv\n",
        "!wget -q -O olist_products.csv https://raw.githubusercontent.com/ayushic2899/Brazilian-E-Commerce-Public-Dataset-by-Olist/master/olist_products_dataset.csv\n",
        "!wget -q -O olist_customers.csv https://raw.githubusercontent.com/ayushic2899/Brazilian-E-Commerce-Public-Dataset-by-Olist/master/olist_customers_dataset.csv\n",
        "\n",
        "print(\"‚úÖ Download Cleaned!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d738d414",
      "metadata": {
        "id": "d738d414"
      },
      "source": [
        "### üõ†Ô∏è Task 1: Load & Clean Data\n",
        "\n",
        "1. ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå CSV ‡∏ó‡∏±‡πâ‡∏á 4 ‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏π‡πà Spark DataFrame\n",
        "2. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Schema ‡πÅ‡∏•‡∏∞‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Type (‡∏ñ‡πâ‡∏≤‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô)\n",
        "3. ‡∏™‡∏£‡πâ‡∏≤‡∏á Temp View ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö SQL\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "931a9d2c",
      "metadata": {
        "id": "931a9d2c"
      },
      "outputs": [],
      "source": [
        "# TODO: Load Data\n",
        "# df_orders = ...\n",
        "# df_items = ...\n",
        "# df_products = ...\n",
        "# df_customers = ...\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5626d3ce",
      "metadata": {
        "id": "5626d3ce"
      },
      "source": [
        "### üîó Task 2: Join Data\n",
        "\n",
        "‡∏à‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á `df_master` ‡πÇ‡∏î‡∏¢‡∏Å‡∏≤‡∏£ Join ‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏î‡∏±‡∏á‡∏ô‡∏µ‡πâ:\n",
        "1. `orders` JOIN `items` (ON order_id)\n",
        "2. JOIN `products` (ON product_id)\n",
        "3. JOIN `customers` (ON customer_id)\n",
        "\n",
        "> **Tip:** ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÅ‡∏ñ‡∏ß‡∏´‡∏•‡∏±‡∏á Join ‡∏ß‡πà‡∏≤‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡∏∂‡πâ‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡∏•‡∏î‡∏•‡∏á‡∏ú‡∏¥‡∏î‡∏õ‡∏Å‡∏ï‡∏¥‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c611bcf",
      "metadata": {
        "id": "5c611bcf"
      },
      "outputs": [],
      "source": [
        "# TODO: Join Data\n",
        "# df_master = ...\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d198e3d",
      "metadata": {
        "id": "2d198e3d"
      },
      "source": [
        "### üìä Task 3: Analytics\n",
        "\n",
        "‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ (‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÉ‡∏ä‡πâ SQL ‡∏´‡∏£‡∏∑‡∏≠ PySpark ‡∏Å‡πá‡πÑ‡∏î‡πâ):\n",
        "\n",
        "1. **Top Products:** ‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡∏´‡∏°‡∏ß‡∏î‡πÑ‡∏´‡∏ô (`product_category_name`) ‡∏°‡∏µ‡∏¢‡∏≠‡∏î‡∏Ç‡∏≤‡∏¢‡∏£‡∏ß‡∏° (price) ‡∏™‡∏π‡∏á‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î 5 ‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö‡πÅ‡∏£‡∏Å?\n",
        "2. **Top Cities:** ‡πÄ‡∏°‡∏∑‡∏≠‡∏á‡πÑ‡∏´‡∏ô (`customer_city`) ‡∏°‡∏µ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏ã‡∏∑‡πâ‡∏≠‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î 5 ‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö‡πÅ‡∏£‡∏Å?\n",
        "3. **(Optional) Monthly Sales:** ‡∏¢‡∏≠‡∏î‡∏Ç‡∏≤‡∏¢‡∏£‡∏ß‡∏°‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£? (‡πÅ‡∏ô‡∏ß‡πÇ‡∏ô‡πâ‡∏°)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24a0682a",
      "metadata": {
        "id": "24a0682a"
      },
      "outputs": [],
      "source": [
        "# TODO: Analytics Code\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aab4fefe",
      "metadata": {
        "id": "aab4fefe"
      },
      "outputs": [],
      "source": [
        "# ‡∏õ‡∏¥‡∏î Spark Session ‡∏´‡∏•‡∏±‡∏á‡∏ó‡∏≥ Lab Test ‡πÄ‡∏™‡∏£‡πá‡∏à\n",
        "spark.stop()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "lecture",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}